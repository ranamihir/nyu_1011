{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Model\n",
    "\n",
    "Today we are going to build a language model using pytorch.  \n",
    "\n",
    "Instead of using word and character tokens, we are going to use **subwords**. Words can be furthur broken down into subword units which usually have meaning.  \n",
    "Example:\n",
    "basketball => basket@@ ball  \n",
    "everyday => every@@ day\n",
    "\n",
    "'@@ ' here denotes continuation of a word.\n",
    "\n",
    "**Byte Pair Encoding (BPE)** creates common tokens that can be used to spilt out-of-vocabulary words.\n",
    "\n",
    "Install subword-nmt for BPE tokenization:\n",
    "    `pip install subword-nmt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn BPE tokens\n",
    "First, we are going to learn 10000 most common units from splitting OOV words using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learn a bpe vocabulary using subword-nmt \n",
    "!subword-nmt learn-bpe -s 10000 < data/ptb.train.txt > data/codes.bpe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply BEP \n",
    "Apply BPE to the train, test and valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply bpe to the data files\n",
    "!subword-nmt apply-bpe -c data/codes.bpe < data/ptb.train.txt > data/ptb.train.bpe.txt\n",
    "!subword-nmt apply-bpe -c data/codes.bpe < data/ptb.test.txt > data/ptb.test.bpe.txt\n",
    "!subword-nmt apply-bpe -c data/codes.bpe < data/ptb.valid.txt > data/ptb.valid.bpe.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the bpe coded file we just created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a@@ er ban@@ kno@@ te ber@@ l@@ it@@ z cal@@ low@@ ay cen@@ trust clu@@ et@@ t fro@@ m@@ stein g@@ it@@ an@@ o gu@@ ter@@ man hy@@ dro@@ -@@ quebec ip@@ o k@@ ia me@@ mo@@ te@@ c m@@ l@@ x na@@ h@@ b p@@ un@@ ts r@@ ake reg@@ att@@ a ru@@ ben@@ s si@@ m sn@@ ac@@ k-@@ food s@@ san@@ gy@@ ong sw@@ ap@@ o w@@ ach@@ ter \r\n",
      " pi@@ er@@ re <unk> N years old will join the board as a non@@ executive director nov. N \r\n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \r\n",
      " ru@@ dol@@ ph <unk> N years old and former chairman of consolidated gold fields plc was named a non@@ executive director of this british industrial conglomerate \r\n",
      " a form of asbestos once used to make k@@ ent cigarette fil@@ ters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \r\n",
      " the asbestos fi@@ ber <unk> is unusually <unk> once it ent@@ ers the <unk> with even brief expo@@ sures to it causing symptoms that show up decades later researchers said \r\n",
      " <unk> inc. the unit of new york-based <unk> corp. that makes k@@ ent cigarettes stopped using <unk> in its <unk> cigarette fil@@ ters in N \r\n",
      " although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a for@@ um likely to bring new attention to the problem \r\n",
      " a <unk> <unk> said this is an old story \r\n",
      " we 're talking about years ago before anyone heard of asbestos having any questionable properties \r\n"
     ]
    }
   ],
   "source": [
    "!head data/ptb.train.bpe.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the data in subword format, we can start writing code for our language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Same as previous labs\n",
    "def tokenize_dataset(dataset): \n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    with open(dataset, 'r') as dataset_file:\n",
    "        for sample in dataset_file:\n",
    "            tokens = sample.strip().split() + ['</s>']\n",
    "            #token_dataset.append(tokens)\n",
    "            all_tokens += tokens\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "val_data = 'data/ptb.valid.bpe.txt'\n",
    "test_data = 'data/ptb.test.bpe.txt'\n",
    "train_data = 'data/ptb.train.bpe.txt'\n",
    "\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_bpe_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_bpe_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_bpe_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 973446\n",
      "Val dataset size is 78354\n",
      "Test dataset size is 85981\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_bpe_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_bpe_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_bpe_tokens.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# same as previous labs\n",
    "max_vocab_size = 20000\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(0,len(vocab)))) \n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(train_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 973446\n",
      "Val dataset size is 78354\n",
      "Test dataset size is 85981\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for token in tokens_data:\n",
    "        token_id = token2id[token] if token in token2id else token2id['<unk>'] \n",
    "        indices_data.append(token_id)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = torch.LongTensor(token2index_dataset(train_data_tokens))\n",
    "val_data_indices = torch.LongTensor(token2index_dataset(val_data_tokens))\n",
    "test_data_indices = torch.LongTensor(token2index_dataset(test_data_tokens))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to learn the language model for the whole training corpus\n",
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "\n",
    "def batchify(data, bsz, random_start_idx=False):\n",
    "    # calculate total number of batches that fit cleanly\n",
    "    nbatch = data.size(0) // bsz\n",
    "    if random_start_idx:\n",
    "        start_idx = random.randint(0, data.size(0) % bsz - 1)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        \n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    # Nice thing about this: \n",
    "    # u don't need to pad since every sequence now has same length\n",
    "    data = data.narrow(0, start_idx, nbatch * bsz)\n",
    "    \n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_batch_size = 32\n",
    "val_data = batchify(val_data_indices, eval_batch_size)\n",
    "test_data = batchify(test_data_indices, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.num_layers, bsz, self.hidden_size),\n",
    "                    weight.new_zeros(self.num_layers, bsz, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 200\n",
    "hidden_size = 400\n",
    "num_layers = 2\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "lr = 0.1\n",
    "dropout = 0.3\n",
    "max_seq_len = 35\n",
    "vocab_size = len(token2id)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of max_seq_len.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# max_seq_len = 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "\n",
    "def get_batch(source, i, max_seq_len):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = 0.3\n",
    "log_interval = 200\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "        Wraps hidden states in new Tensors, to detach them from their history.\n",
    "    \"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    # We shuffle train data every epoch\n",
    "    train_data = batchify(train_data_indices, batch_size, random_start_idx=True)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, max_seq_len)):\n",
    "        data, targets = get_batch(train_data, i, max_seq_len)\n",
    "        \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch %log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            \n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // max_seq_len, lr,\n",
    "                cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perplexity evaluation for a given corpus\n",
    "def evaluate(data_source, max_seq_len, eval_batch_size=32):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, max_seq_len):\n",
    "            data, targets = get_batch(data_source, i, max_seq_len)\n",
    "            \n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, vocab_size)\n",
    "            \n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "best_val_loss = np.inf\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    train()\n",
    "    val_loss = evaluate(val_data, max_seq_len)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, \n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open('model.pt', 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's gonna take a while to train this so we provide the pre-trained file in the zip folder. \n",
    "Let's load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phumon/py3torch/lib/python3.6/site-packages/torch/serialization.py:333: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "with open('model.pt', 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the perplexity on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the perplexity of pre-trained model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity:  157.99914538908837\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(test_data, max_seq_len)\n",
    "print(\"test perplexity: \", math.exp(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the perplexity on the test set using pre-trained KenLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-train KenLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "# calculate perplexity for KenLM\n",
    "def get_ppl(lm, sentences):\n",
    "    \"\"\"\n",
    "    Assume sentences is a list of strings (space delimited sentences)\n",
    "    \"\"\"\n",
    "    total_nll = 0\n",
    "    total_wc = 0\n",
    "    ppl_list = []\n",
    "    for sent in sentences:\n",
    "        words = sent.strip().split()\n",
    "        score = lm.score(sent, bos=False, eos=False)\n",
    "        word_count = len(words)\n",
    "        if word_count <=0:\n",
    "            continue\n",
    "        total_wc += word_count\n",
    "        total_nll += score\n",
    "        sent_ppl = 10**(-score/word_count)\n",
    "        ppl_list.append((sent, sent_ppl))\n",
    "    ppl = 10**-(total_nll/total_wc)\n",
    "    return ppl, ppl_list\n",
    "\n",
    "with open('data/ptb.test.nounk.txt', 'r') as f:\n",
    "    sentences = [sent.strip() for sent in f]\n",
    "    \n",
    "kenlm_model = kenlm.LanguageModel('ptb_lm_2gram.arpa')\n",
    "kenlm_ppl, sent_ppl_list = get_ppl(kenlm_model, sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test ppl (KenLM):  260.13758257335195\n"
     ]
    }
   ],
   "source": [
    "print(\"total test ppl (KenLM): \", kenlm_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"no it was n't black monday\", 244.2144113575105),\n",
       " (\"but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos\",\n",
       "  98.82167725339345),\n",
       " ('some circuit breakers installed after the october N crash failed their first test traders say unable to cool the selling panic in both stocks and futures',\n",
       "  437.0089302511858),\n",
       " (\"the N stock specialist firms on the big board floor the buyers and sellers of last resort who were criticized after the N crash once again could n't handle the selling pressure\",\n",
       "  208.62356173241537),\n",
       " ('big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock traders say',\n",
       "  189.5969019138085)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_ppl_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for getting the perplexity of each sentence in test set\n",
    "def test(sent_list):\n",
    "    ppl_list = []\n",
    "    for sent in sent_list:\n",
    "        tokens = token2index_dataset(sent.strip().split())\n",
    "        test_sent_idx = batchify(torch.LongTensor([tokens]), 1)\n",
    "        loss = evaluate(test_sent_idx, len(tokens), 1)\n",
    "        ppl_list.append((sent, math.exp(loss)))\n",
    "    return ppl_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score sentences with RNN_LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dividend yields have been bolstered by stock declines', 223.58090727154595),\n",
       " ('stock bolstered declines dividend by yields have been', 799.5039174923857),\n",
       " ('artificial neural networks are computing systems vaguely inspired by the biological neural networks',\n",
       "  210.4940782094278)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = ['dividend yields have been bolstered by stock declines', \\\n",
    "             'stock bolstered declines dividend by yields have been', \\\n",
    "             'artificial neural networks are computing systems vaguely inspired by the biological neural networks']\n",
    "test(sent_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the same list of sentences with KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dividend yields have been bolstered by stock declines', 466.40009558112047),\n",
       " ('stock bolstered declines dividend by yields have been', 1818.5723239644926),\n",
       " ('artificial neural networks are computing systems vaguely inspired by the biological neural networks',\n",
       "  9918.597743710336)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ppl, sent_list_ppl = get_ppl(kenlm_model, sent_list)\n",
    "sent_list_ppl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **RNN LM can generalize well and assign lower perplexity** of grammatically correct, out-of-domain sentence, which is not the case with KenLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Find the perplexity of all sentences in test set using RNN_LM and KenLM. Compare the 10 sentences with lowest and highest perplexity produced by each model. Analyze what kind of sentences are preferred by each model.\n",
    "\n",
    "2. Train the character level language model and compare the performance.  \n",
    "\n",
    "2. Create an autocomplete function using the pretrained language model. Example, given a partial sentence, predict the next word.  \n",
    "\n",
    "3. What is the perplexity if your language model always output uniform distribution, i.e your language model assigns the equal probability to all the tokens in the vocabulary.  \n",
    "\n",
    "4. Build a convolutional language model. (Reference: https://arxiv.org/abs/1612.08083)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
