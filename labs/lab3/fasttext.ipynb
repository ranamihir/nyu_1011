{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from: http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# To be used for pre-processing of data\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "First, let's load the dataset from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroup_train = fetch_20newsgroups(subset='train')\n",
    "newsgroup_test = fetch_20newsgroups(subset='test') # we will use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 10000\n",
      "Val dataset size is 1314\n",
      "Test dataset size is 7532\n"
     ]
    }
   ],
   "source": [
    "train_split = 10000\n",
    "train_data = newsgroup_train.data[:train_split]\n",
    "train_targets = newsgroup_train.target[:train_split]\n",
    "\n",
    "val_data = newsgroup_train.data[train_split:]\n",
    "val_targets = newsgroup_train.target[train_split:]\n",
    "\n",
    "test_data = newsgroup_test.data\n",
    "test_targets = newsgroup_test.target\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext library takes a file as input and learn a classification model.\n",
    "The sentences in input file should be in this format: \"_ __label__ _[class] [Text]\" \n",
    "We will prepare the train file and test file in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_newsgroup_file(data, targets, outfile_name):\n",
    "    with open(outfile_name, 'w') as fout:\n",
    "        for i, sent in enumerate(data):\n",
    "            line = \"__label__\" + str(targets[i]) + \" \" + sent.replace('\\n', ' ') + \"\\n\"\n",
    "            fout.write(line)\n",
    "            \n",
    "\n",
    "create_newsgroup_file(train_data, train_targets, 'newsgroups.train') \n",
    "create_newsgroup_file(val_data, val_targets, 'newsgroups.val') \n",
    "create_newsgroup_file(test_data, test_targets, 'newsgroups.test') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how the file we created look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__7 From: lerxst@wam.umd.edu (where's my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15   I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is  all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail.  Thanks, - IL    ---- brought to you by your neighborhood Lerxst ----     \r\n",
      "__label__4 From: guykuo@carson.u.washington.edu (Guy Kuo) Subject: SI Clock Poll - Final Call Summary: Final call for SI clock reports Keywords: SI,acceleration,clock,upgrade Article-I.D.: shelley.1qvfo9INNc3s Organization: University of Washington Lines: 11 NNTP-Posting-Host: carson.u.washington.edu  A fair number of brave souls who upgraded their SI clock oscillator have shared their experiences for this poll. Please send a brief message detailing your experiences with the procedure. Top speed attained, CPU rated speed, add on cards and adapters, heat sinks, hour of usage per day, floppy disk functionality with 800 and 1.4 m floppies are especially requested.  I will be summarizing in the next two days, so please add to the network knowledge base if you have done the clock upgrade and haven't answered this poll. Thanks.  Guy Kuo <guykuo@u.washington.edu> \r\n"
     ]
    }
   ],
   "source": [
    "!head -2 newsgroups.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install FastText if you haven't! \n",
    "Use the following commands to install fasttext.\n",
    "```\n",
    "wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip\n",
    "unzip v0.1.0.zip\n",
    "cd fastText-0.1.0\n",
    "make\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start training the fasttext classifier, and check its performance on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  258366\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 3067690  lr: 0.000000  loss: 3.000534  eta: 0h0m ords/sec/thread: 2059702  lr: 0.099346  loss: 2.841834  eta: 0h0m 0m m gress: 45.7%  words/sec/thread: 3066358  lr: 0.054282  loss: 3.017017  eta: 0h0m 3053475  lr: 0.050331  loss: 3.016595  eta: 0h0m gress: 59.4%  words/sec/thread: 3059738  lr: 0.040556  loss: 3.014911  eta: 0h0m gress: 78.1%  words/sec/thread: 3064380  lr: 0.021868  loss: 3.009293  eta: 0h0m 7953  loss: 3.008615  eta: 0h0m %  words/sec/thread: 3062455  lr: 0.014456  loss: 3.005642  eta: 0h0m gress: 99.4%  words/sec/thread: 3067650  lr: 0.000616  loss: 3.000534  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "# Train fasttext\n",
    "!./fastText-0.1.0/fastText supervised -input newsgroups.train -output model_newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1314\r\n",
      "P@1\t0.101\r\n",
      "R@1\t0.101\r\n",
      "Number of examples: 1314\r\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it on validation set\n",
    "!./fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that FastText reports the precision and recall, not accuracy!  \n",
    "The **precision** is the number of correct labels among the labels predicted by fastText.  \n",
    "The **recall** is the number of labels that successfully were predicted, among all the real labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What a horrible model! Do some preprocessing to make it better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from lerxst@wam.umd.edu where 's my thing subject what car is this nntp posting host rac3.wam.umd.edu organization university of maryland college park lines 15    i was wondering if anyone out there could enlighten me on this car i saw the other day it was a 2-door sports car looked to be from the late 60s/ early 70s it was called a bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is   all i know if anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please e mail   thanks il     ---- brought to you by your neighborhood lerxst ----\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sent(sent):\n",
    "    temp_sent = ' '.join(sent.split('\\n')) # remove line breaks as fasttext read each sample text as a line\n",
    "    tokens = tokenizer(temp_sent)\n",
    "    pos = [(tok.text, tok.pos_) for tok in tokens]\n",
    "    processed_toks = [tok.text.lower() for tok in tokens if (tok.text not in punctuations)]\n",
    "    \n",
    "    return ' '.join(processed_toks).strip() #[token.text.lower() for token in tokens]\n",
    "    \n",
    "    \n",
    "temp = preprocess_sent(train_data[0])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_newsgroup_file(data, targets, outfile_name):\n",
    "    with open(outfile_name, 'w') as fout:\n",
    "        for i, sent in enumerate(data):\n",
    "            proc_sent = preprocess_sent(sent)\n",
    "            line = \"__label__\" + str(targets[i]) + \" \" + proc_sent + \"\\n\"\n",
    "            fout.write(line)\n",
    "            \n",
    "create_newsgroup_file(train_data, train_targets, 'newsgroups.proc.train') \n",
    "create_newsgroup_file(val_data, val_targets, 'newsgroups.proc.val') \n",
    "create_newsgroup_file(test_data, test_targets, 'newsgroups.proc.test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134300\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 3659460  lr: 0.000000  loss: 2.972613  eta: 0h0m 14m words/sec/thread: 1560763  lr: 0.099874  loss: 2.841834  eta: 0h0m  words/sec/thread: 3509511  lr: 0.096054  loss: 2.957827  eta: 0h0m m   words/sec/thread: 3547130  lr: 0.074283  loss: 3.016389  eta: 0h0m gress: 37.7%  words/sec/thread: 3647125  lr: 0.062277  loss: 3.016427  eta: 0h0m 0m %  words/sec/thread: 3619299  lr: 0.043019  loss: 3.008533  eta: 0h0m s: 68.6%  words/sec/thread: 3663936  lr: 0.031382  loss: 3.004586  eta: 0h0m gress: 73.0%  words/sec/thread: 3668848  lr: 0.026969  loss: 3.000934  eta: 0h0m gress: 79.9%  words/sec/thread: 3662178  lr: 0.020098  loss: 2.994315  eta: 0h0m s: 84.6%  words/sec/thread: 3653509  lr: 0.015363  loss: 2.985764  eta: 0h0m s: 88.3%  words/sec/thread: 3648488  lr: 0.011748  loss: 2.977858  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "!./fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1314\r\n",
      "P@1\t0.117\r\n",
      "R@1\t0.117\r\n",
      "Number of examples: 1314\r\n"
     ]
    }
   ],
   "source": [
    "!./fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see tiny improvement but still a bad model. Let's adjust the hyperparameters of the model.\n",
    "Fasttext library uses 5 training epochs by default, which is not enough for learning our data. \n",
    "Let's try adjusting the number of epoch to 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is important to note that the two models above aren't strictly comparable.\n",
    "Each model is randomly initialized at the beginning of the training. So, every time you re-train the model, you will notice that the precision and recall are different.\n",
    "In practice, it's a good idea to train the model with different initializations at least 5 times, and report the min, max, mean, and median stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134300\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 3587300  lr: 0.000000  loss: 1.237170  eta: 0h0m 14m   words/sec/thread: 3557694  lr: 0.096855  loss: 3.013989  eta: 0h0m ead: 3597244  lr: 0.096162  loss: 3.015560  eta: 0h0m h0m  words/sec/thread: 3558857  lr: 0.095185  loss: 3.017139  eta: 0h0m thread: 3643371  lr: 0.093219  loss: 3.014914  eta: 0h0m m   lr: 0.092329  loss: 3.011415  eta: 0h0m   lr: 0.090072  loss: 2.989636  eta: 0h0m 0.089579  loss: 2.954622  eta: 0h0m   words/sec/thread: 3454690  lr: 0.088291  loss: 2.929592  eta: 0h0m 13.2%  words/sec/thread: 3444781  lr: 0.086831  loss: 2.909479  eta: 0h0m 9874  eta: 0h0m   eta: 0h0m gress: 17.4%  words/sec/thread: 3495630  lr: 0.082558  loss: 2.815600  eta: 0h0m s: 17.8%  words/sec/thread: 3504977  lr: 0.082157  loss: 2.813912  eta: 0h0m   loss: 2.748497  eta: 0h0m ss: 2.746757  eta: 0h0m gress: 21.3%  words/sec/thread: 3517643  lr: 0.078666  loss: 2.717894  eta: 0h0m   loss: 2.620352  eta: 0h0m gress: 24.4%  words/sec/thread: 3531569  lr: 0.075615  loss: 2.606289  eta: 0h0m gress: 26.3%  words/sec/thread: 3541081  lr: 0.073746  loss: 2.583623  eta: 0h0m h0m gress: 32.0%  words/sec/thread: 3565201  lr: 0.068022  loss: 2.412929  eta: 0h0m   words/sec/thread: 3574931  lr: 0.065832  loss: 2.340940  eta: 0h0m gress: 37.0%  words/sec/thread: 3582795  lr: 0.063011  loss: 2.265361  eta: 0h0m s: 37.7%  words/sec/thread: 3585089  lr: 0.062268  loss: 2.241163  eta: 0h0m   loss: 2.229686  eta: 0h0m gress: 40.3%  words/sec/thread: 3579305  lr: 0.059688  loss: 2.149549  eta: 0h0m gress: 43.8%  words/sec/thread: 3583857  lr: 0.056234  loss: 2.073334  eta: 0h0m   eta: 0h0m 5.2%  words/sec/thread: 3587522  lr: 0.054836  loss: 2.048427  eta: 0h0m %  words/sec/thread: 3589118  lr: 0.053433  loss: 2.021415  eta: 0h0m s: 47.2%  words/sec/thread: 3587494  lr: 0.052820  loss: 1.986829  eta: 0h0m gress: 48.5%  words/sec/thread: 3589448  lr: 0.051506  loss: 1.979144  eta: 0h0m 8286  eta: 0h0m gress: 53.9%  words/sec/thread: 3591292  lr: 0.046132  loss: 1.833371  eta: 0h0m s: 55.7%  words/sec/thread: 3596778  lr: 0.044327  loss: 1.815876  eta: 0h0m %  words/sec/thread: 3598798  lr: 0.043494  loss: 1.812372  eta: 0h0m gress: 57.2%  words/sec/thread: 3593986  lr: 0.042835  loss: 1.773665  eta: 0h0m m 63  loss: 1.692703  eta: 0h0m sec/thread: 3589331  lr: 0.036704  loss: 1.667674  eta: 0h0m 4.9%  words/sec/thread: 3589744  lr: 0.035067  loss: 1.638361  eta: 0h0m 1.609404  eta: 0h0m 0h0m 9.1%  words/sec/thread: 3590826  lr: 0.030883  loss: 1.583930  eta: 0h0m   words/sec/thread: 3589204  lr: 0.030333  loss: 1.562318  eta: 0h0m rds/sec/thread: 3589512  lr: 0.030312  loss: 1.561404  eta: 0h0m sec/thread: 3592213  lr: 0.028792  loss: 1.560474  eta: 0h0m h0m gress: 72.7%  words/sec/thread: 3585267  lr: 0.027309  loss: 1.519959  eta: 0h0m gress: 75.2%  words/sec/thread: 3591544  lr: 0.024807  loss: 1.508048  eta: 0h0m ess: 77.1%  words/sec/thread: 3588691  lr: 0.022949  loss: 1.476883  eta: 0h0m  lr: 0.022589  loss: 1.464293  eta: 0h0m  0.021519  loss: 1.463112  eta: 0h0m gress: 79.8%  words/sec/thread: 3588418  lr: 0.020217  loss: 1.445351  eta: 0h0m 0.018860  loss: 1.434776  eta: 0h0m 1.9%  words/sec/thread: 3590604  lr: 0.018070  loss: 1.416497  eta: 0h0m %  words/sec/thread: 3588534  lr: 0.017769  loss: 1.403392  eta: 0h0m h0m gress: 86.8%  words/sec/thread: 3588186  lr: 0.013153  loss: 1.357544  eta: 0h0m gress: 89.8%  words/sec/thread: 3588226  lr: 0.010182  loss: 1.322884  eta: 0h0m gress: 91.6%  words/sec/thread: 3588278  lr: 0.008435  loss: 1.315851  eta: 0h0m gress: 93.0%  words/sec/thread: 3586545  lr: 0.007037  loss: 1.285939  eta: 0h0m s: 93.0%  words/sec/thread: 3585937  lr: 0.007022  loss: 1.285441  eta: 0h0m gress: 94.3%  words/sec/thread: 3588653  lr: 0.005668  loss: 1.284044  eta: 0h0m gress: 95.0%  words/sec/thread: 3587673  lr: 0.004984  loss: 1.278681  eta: 0h0m gress: 96.8%  words/sec/thread: 3585453  lr: 0.003217  loss: 1.259618  eta: 0h0m h0m \n",
      "N\t1314\n",
      "P@1\t0.785\n",
      "R@1\t0.785\n",
      "Number of examples: 1314\n"
     ]
    }
   ],
   "source": [
    "!./fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup -epoch 30\n",
    "!./fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! A huge improvement. \n",
    "Learning rate dictates how fast a model learns. By default, it's 0.05. Model will converge faster with bigger learning rate, though bigger learning rate doesn't always mean better.\n",
    "Let's adjust it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134300\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 3610860  lr: 0.000000  loss: 0.239296  eta: 0h0m m  words/sec/thread: 3708512  lr: 0.470649  loss: 2.381975  eta: 0h0m 0.466045  loss: 2.289226  eta: 0h0m  words/sec/thread: 3689278  lr: 0.459338  loss: 2.115019  eta: 0h0m gress: 10.3%  words/sec/thread: 3674785  lr: 0.448627  loss: 1.827468  eta: 0h0m gress: 10.8%  words/sec/thread: 3669020  lr: 0.445983  loss: 1.721792  eta: 0h0m 0h0m %  words/sec/thread: 3657712  lr: 0.438591  loss: 1.492550  eta: 0h0m gress: 17.1%  words/sec/thread: 3651927  lr: 0.414698  loss: 1.235371  eta: 0h0m 8.5%  words/sec/thread: 3650562  lr: 0.407338  loss: 1.084880  eta: 0h0m   eta: 0h0m s: 21.7%  words/sec/thread: 3671343  lr: 0.391332  loss: 0.998239  eta: 0h0m gress: 23.1%  words/sec/thread: 3673424  lr: 0.384381  loss: 0.954280  eta: 0h0m 2778  eta: 0h0m  gress: 24.8%  words/sec/thread: 3656970  lr: 0.375955  loss: 0.869884  eta: 0h0m gress: 27.5%  words/sec/thread: 3653536  lr: 0.362606  loss: 0.773139  eta: 0h0m s: 29.8%  words/sec/thread: 3668126  lr: 0.351153  loss: 0.762442  eta: 0h0m gress: 30.3%  words/sec/thread: 3659255  lr: 0.348489  loss: 0.730999  eta: 0h0m gress: 31.5%  words/sec/thread: 3652984  lr: 0.342353  loss: 0.719634  eta: 0h0m gress: 32.5%  words/sec/thread: 3648435  lr: 0.337661  loss: 0.683977  eta: 0h0m gress: 34.8%  words/sec/thread: 3641937  lr: 0.325890  loss: 0.653882  eta: 0h0m gress: 35.5%  words/sec/thread: 3640482  lr: 0.322423  loss: 0.629604  eta: 0h0m gress: 35.7%  words/sec/thread: 3639784  lr: 0.321433  loss: 0.623413  eta: 0h0m s: 39.7%  words/sec/thread: 3636896  lr: 0.301603  loss: 0.578392  eta: 0h0m gress: 40.3%  words/sec/thread: 3633694  lr: 0.298531  loss: 0.559236  eta: 0h0m gress: 41.6%  words/sec/thread: 3637620  lr: 0.291976  loss: 0.552163  eta: 0h0m gress: 43.6%  words/sec/thread: 3628234  lr: 0.282232  loss: 0.522840  eta: 0h0m gress: 45.2%  words/sec/thread: 3628750  lr: 0.274049  loss: 0.498571  eta: 0h0m s: 46.3%  words/sec/thread: 3627503  lr: 0.268453  loss: 0.497842  eta: 0h0m gress: 46.8%  words/sec/thread: 3629130  lr: 0.265828  loss: 0.492177  eta: 0h0m gress: 48.3%  words/sec/thread: 3626520  lr: 0.258489  loss: 0.470927  eta: 0h0m   loss: 0.470238  eta: 0h0m gress: 49.6%  words/sec/thread: 3626819  lr: 0.251753  loss: 0.467668  eta: 0h0m %  words/sec/thread: 3619353  lr: 0.245649  loss: 0.450203  eta: 0h0m gress: 51.2%  words/sec/thread: 3617082  lr: 0.243785  loss: 0.439355  eta: 0h0m s: 52.9%  words/sec/thread: 3620298  lr: 0.235612  loss: 0.438693  eta: 0h0m s: 53.3%  words/sec/thread: 3616744  lr: 0.233321  loss: 0.435780  eta: 0h0m gress: 55.3%  words/sec/thread: 3615091  lr: 0.223395  loss: 0.421394  eta: 0h0m   words/sec/thread: 3608635  lr: 0.215074  loss: 0.413269  eta: 0h0m 57.9%  words/sec/thread: 3598720  lr: 0.210622  loss: 0.396864  eta: 0h0m gress: 59.2%  words/sec/thread: 3602591  lr: 0.203875  loss: 0.394902  eta: 0h0m   lr: 0.197332  loss: 0.385053  eta: 0h0m gress: 61.1%  words/sec/thread: 3598228  lr: 0.194380  loss: 0.372404  eta: 0h0m gress: 63.7%  words/sec/thread: 3591594  lr: 0.181472  loss: 0.361692  eta: 0h0m %  words/sec/thread: 3584224  lr: 0.163535  loss: 0.343408  eta: 0h0m gress: 70.7%  words/sec/thread: 3594492  lr: 0.146461  loss: 0.326135  eta: 0h0m gress: 71.7%  words/sec/thread: 3597661  lr: 0.141748  loss: 0.325490  eta: 0h0m a: 0h0m gress: 73.8%  words/sec/thread: 3600792  lr: 0.131233  loss: 0.316917  eta: 0h0m gress: 75.1%  words/sec/thread: 3594633  lr: 0.124315  loss: 0.311481  eta: 0h0m  gress: 80.1%  words/sec/thread: 3594340  lr: 0.099642  loss: 0.293419  eta: 0h0m %  words/sec/thread: 3596355  lr: 0.095280  loss: 0.288455  eta: 0h0m ords/sec/thread: 3596099  lr: 0.095054  loss: 0.288255  eta: 0h0m gress: 82.8%  words/sec/thread: 3602237  lr: 0.086142  loss: 0.284922  eta: 0h0m gress: 82.9%  words/sec/thread: 3602433  lr: 0.085304  loss: 0.283524  eta: 0h0m 0.067462  loss: 0.271279  eta: 0h0m 1411  loss: 0.271075  eta: 0h0m gress: 88.4%  words/sec/thread: 3596969  lr: 0.057868  loss: 0.270024  eta: 0h0m 89.9%  words/sec/thread: 3597493  lr: 0.050396  loss: 0.265317  eta: 0h0m gress: 90.7%  words/sec/thread: 3599383  lr: 0.046447  loss: 0.262109  eta: 0h0m   words/sec/thread: 3600644  lr: 0.042527  loss: 0.259237  eta: 0h0m gress: 91.8%  words/sec/thread: 3600360  lr: 0.041204  loss: 0.257743  eta: 0h0m rds/sec/thread: 3608468  lr: 0.028877  loss: 0.254336  eta: 0h0m gress: 95.0%  words/sec/thread: 3609437  lr: 0.025129  loss: 0.251408  eta: 0h0m 0642  lr: 0.021266  loss: 0.248503  eta: 0h0m sec/thread: 3610014  lr: 0.008861  loss: 0.241795  eta: 0h0m 98.5%  words/sec/thread: 3609790  lr: 0.007265  loss: 0.239296  eta: 0h0m \n",
      "N\t1314\n",
      "P@1\t0.868\n",
      "R@1\t0.868\n",
      "Number of examples: 1314\n"
     ]
    }
   ],
   "source": [
    "!./fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup -epoch 30 -lr 0.5\n",
    "!./fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, the results improves! \n",
    "\n",
    "Now, instead of using **bags of words**, let's try using **bags of N-grams**. We'll use **Bigrams (N=2)** here.  \n",
    "N-grams provide a sense of word order. \n",
    "\n",
    "Sentence: \"I love eating pizza\"  \n",
    "Bigrams for the above sentence: \"I love\", \"love eating\", \"eating pizza\".  \n",
    "By looking at the N-grams, it is possible to reconstruct a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134300\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 1577546  lr: 0.000000  loss: 0.394258  eta: 0h0m 14m d: 1521699  lr: 0.496238  loss: 2.994074  eta: 0h0m 0.491502  loss: 3.012293  eta: 0h0m 2.4%  words/sec/thread: 1564740  lr: 0.488079  loss: 3.013380  eta: 0h0m  words/sec/thread: 1581270  lr: 0.482903  loss: 2.998689  eta: 0h0m m h0m  words/sec/thread: 1607165  lr: 0.463237  loss: 2.786102  eta: 0h0m 0.460744  loss: 2.736167  eta: 0h0m  words/sec/thread: 1598420  lr: 0.457409  loss: 2.669891  eta: 0h0m  words/sec/thread: 1603732  lr: 0.455483  loss: 2.653238  eta: 0h0m d: 1601958  lr: 0.450499  loss: 2.544457  eta: 0h0m 10.2%  words/sec/thread: 1603242  lr: 0.448826  loss: 2.521739  eta: 0h0m gress: 11.2%  words/sec/thread: 1599043  lr: 0.443750  loss: 2.363167  eta: 0h0m %  words/sec/thread: 1601378  lr: 0.440802  loss: 2.352304  eta: 0h0m gress: 14.7%  words/sec/thread: 1597748  lr: 0.426492  loss: 2.118769  eta: 0h0m 15.2%  words/sec/thread: 1596342  lr: 0.423842  loss: 2.082696  eta: 0h0m gress: 18.8%  words/sec/thread: 1597432  lr: 0.405907  loss: 1.815871  eta: 0h0m s: 19.3%  words/sec/thread: 1598134  lr: 0.403652  loss: 1.774181  eta: 0h0m gress: 19.8%  words/sec/thread: 1596847  lr: 0.401039  loss: 1.752224  eta: 0h0m m gress: 23.2%  words/sec/thread: 1593517  lr: 0.383991  loss: 1.554899  eta: 0h0m gress: 24.2%  words/sec/thread: 1594693  lr: 0.379030  loss: 1.505771  eta: 0h0m h0m   lr: 0.360084  loss: 1.319994  eta: 0h0m 1594672  lr: 0.354558  loss: 1.280997  eta: 0h0m gress: 29.4%  words/sec/thread: 1593491  lr: 0.353073  loss: 1.255920  eta: 0h0m gress: 31.0%  words/sec/thread: 1593797  lr: 0.345013  loss: 1.203400  eta: 0h0m s: 32.2%  words/sec/thread: 1592495  lr: 0.338926  loss: 1.162416  eta: 0h0m : 0.337613  loss: 1.155559  eta: 0h0m gress: 34.0%  words/sec/thread: 1588695  lr: 0.330099  loss: 1.099928  eta: 0h0m gress: 34.7%  words/sec/thread: 1588890  lr: 0.326448  loss: 1.081542  eta: 0h0m gress: 38.4%  words/sec/thread: 1587285  lr: 0.308199  loss: 0.972190  eta: 0h0m 365  lr: 0.296532  loss: 0.918599  eta: 0h0m a: 0h0m gress: 44.8%  words/sec/thread: 1585912  lr: 0.276028  loss: 0.839542  eta: 0h0m gress: 49.9%  words/sec/thread: 1586170  lr: 0.250409  loss: 0.757828  eta: 0h0m 0h0m s: 53.5%  words/sec/thread: 1585419  lr: 0.232476  loss: 0.717774  eta: 0h0m 1584787  lr: 0.230978  loss: 0.712269  eta: 0h0m   eta: 0h0m 56.5%  words/sec/thread: 1584639  lr: 0.217672  loss: 0.674048  eta: 0h0m gress: 57.6%  words/sec/thread: 1585120  lr: 0.212098  loss: 0.665266  eta: 0h0m   loss: 0.644057  eta: 0h0m gress: 61.3%  words/sec/thread: 1584657  lr: 0.193577  loss: 0.631069  eta: 0h0m %  words/sec/thread: 1583755  lr: 0.190782  loss: 0.624603  eta: 0h0m gress: 62.0%  words/sec/thread: 1584139  lr: 0.189917  loss: 0.623882  eta: 0h0m s: 62.5%  words/sec/thread: 1584501  lr: 0.187522  loss: 0.620461  eta: 0h0m gress: 63.0%  words/sec/thread: 1583638  lr: 0.185153  loss: 0.616196  eta: 0h0m gress: 63.6%  words/sec/thread: 1583105  lr: 0.182172  loss: 0.611266  eta: 0h0m s: 68.7%  words/sec/thread: 1582890  lr: 0.156383  loss: 0.570776  eta: 0h0m gress: 69.4%  words/sec/thread: 1583029  lr: 0.152876  loss: 0.565488  eta: 0h0m 1582909  lr: 0.141459  loss: 0.548442  eta: 0h0m ords/sec/thread: 1582795  lr: 0.140001  loss: 0.545062  eta: 0h0m gress: 73.2%  words/sec/thread: 1583148  lr: 0.133925  loss: 0.534687  eta: 0h0m 74.1%  words/sec/thread: 1582964  lr: 0.129461  loss: 0.527758  eta: 0h0m s: 74.8%  words/sec/thread: 1583026  lr: 0.125763  loss: 0.523634  eta: 0h0m 0.125229  loss: 0.522588  eta: 0h0m gress: 75.3%  words/sec/thread: 1583230  lr: 0.123358  loss: 0.519470  eta: 0h0m gress: 77.4%  words/sec/thread: 1582694  lr: 0.113199  loss: 0.506890  eta: 0h0m gress: 80.2%  words/sec/thread: 1582730  lr: 0.098784  loss: 0.487336  eta: 0h0m 0h0m m 0h0m rogress: 84.4%  words/sec/thread: 1582839  lr: 0.077833  loss: 0.460515  eta: 0h0m gress: 86.1%  words/sec/thread: 1582891  lr: 0.069655  loss: 0.455792  eta: 0h0m gress: 86.7%  words/sec/thread: 1580735  lr: 0.066500  loss: 0.452564  eta: 0h0m gress: 88.2%  words/sec/thread: 1578680  lr: 0.058896  loss: 0.445572  eta: 0h0m gress: 89.3%  words/sec/thread: 1578408  lr: 0.053352  loss: 0.439943  eta: 0h0m /sec/thread: 1577994  lr: 0.050332  loss: 0.436661  eta: 0h0m gress: 90.1%  words/sec/thread: 1578134  lr: 0.049556  loss: 0.434865  eta: 0h0m   words/sec/thread: 1577771  lr: 0.029017  loss: 0.418266  eta: 0h0m gress: 96.6%  words/sec/thread: 1578070  lr: 0.016885  loss: 0.406919  eta: 0h0m rds/sec/thread: 1577897  lr: 0.015901  loss: 0.404328  eta: 0h0m 2808  eta: 0h0m   loss: 0.402523  eta: 0h0m 98.2%  words/sec/thread: 1578072  lr: 0.008781  loss: 0.400086  eta: 0h0m \n",
      "N\t1314\n",
      "P@1\t0.868\n",
      "R@1\t0.868\n",
      "Number of examples: 1314\n"
     ]
    }
   ],
   "source": [
    "!./fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup \\\n",
    "-epoch 30 -lr 0.5 -wordNgrams 2\n",
    "!./fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check out other hyperparameters you can adjust on the Fasttext repo: https://github.com/facebookresearch/fastText/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have chosen the best model based on validation performance, we can test how it perform on actual test set.  \n",
    "Remember the lecture? ***Never*** tune your model on test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t7532\r\n",
      "P@1\t0.764\r\n",
      "R@1\t0.764\r\n",
      "Number of examples: 7532\r\n"
     ]
    }
   ],
   "source": [
    "!./fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Try training the fastText using IMDB Large Movie Review Dataset and fine-tune the hyperparameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
