{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling: KenLM\n",
    "\n",
    "We are going to learn how to use KenLM, a toolkit for language modeling.\n",
    "First of all, you need to install KenLM.\n",
    "- Download and unzip: http://kheafield.com/code/kenlm.tar.gz\n",
    "- You need:\n",
    "    - cmake : https://cmake.org/download/ and unzip.\n",
    "      - Do the following:\n",
    "       ``` cd cmake\n",
    "           ./bootstrap\n",
    "           make\n",
    "           make install\n",
    "       ```\n",
    "    - Need Boost >= 1.42.0 and bjam\n",
    "        - Ubuntu: sudo apt-get install libboost-all-dev\n",
    "        - Mac: brew install boost; brew install bjam\n",
    "- cd into kenlm folder and compiling using the following commands:\n",
    "```bash\n",
    "mkdir -p build\n",
    "cd build\n",
    "cmake ..\n",
    "make -j 4\n",
    "```\n",
    "- Install python KenLM: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "- Check out KenLM website for more info: http://kheafield.com/code/kenlm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a language model with KenLM\n",
    "Let's train a bigram language model and 4-gram language model.  \n",
    "First, download the preprocessed Penn Treebank (Wall Street Journal) dataset from here: https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data.\n",
    "KenLM doesn't support <unk> token so let's remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This removes all occurences of <unk> tokens\n",
    "# sed is a very handy command for quick processing.  \n",
    "# I strongly recommend you to learn how to use it. \n",
    "# https://www.tutorialspoint.com/sed/sed_overview.htm\n",
    "!sed -e 's/<unk>//g' data/ptb.train.txt > data/ptb.train.nounk.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bigram\n",
    "!./kenlm/build/bin/lmplz -o 2 < data/ptb.train.nounk.txt > ptb_lm_2gram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4-gram\n",
    "!./kenlm/build/bin/lmplz -o 4 < data/ptb.train.nounk.txt > ptb_lm_4gram.arpa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring using KenLM\n",
    "Let's score a sentence using the language model we just trained.  \n",
    "**Note that the score KenLM returns is log likelihood, not perplexity!**  \n",
    "Pereplexity is defined as follow: $$ PPL = b^{- \\frac{1}{N} \\sum_{i=1}^N \\log_b q(x_i)} $$  \n",
    "\n",
    "All probabilities here are in log base 10 so to convert to perplexity, we do the following:  \n",
    "$$PPL = 10^{-\\log(P) / N} $$\n",
    "where $-\\log(P)$ is the total NLL of the whole sentence, and $N$ is the word count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the pre-trained LMs\n",
    "bigram_model = kenlm.LanguageModel('ptb_lm_2gram.arpa')\n",
    "trigram_model = kenlm.LanguageModel('ptb_lm_4gram.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for calculating perplexity\n",
    "def get_ppl(model, sent):\n",
    "    return 10**(-model.score(sent)/len(sent.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"dividend yields have been bolstered by stock declines \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL of a sentence from PTB test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.9773725405043\n",
      "733.1557213309632\n"
     ]
    }
   ],
   "source": [
    "print(get_ppl(bigram_model, sentence))\n",
    "print(get_ppl(trigram_model, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL of an out-of-domain sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13349.78268920608\n",
      "13699.961190363858\n"
     ]
    }
   ],
   "source": [
    "ood_sentence = 'artificial neural networks are computing systems vaguely inspired by the biological neural networks'\n",
    "print(get_ppl(bigram_model, ood_sentence))\n",
    "print(get_ppl(trigram_model, ood_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the sentence from test set to get novel N-grams and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock bolstered declines dividend by yields have been\n",
      "3207.5970808942507\n",
      "3302.2615231292616\n"
     ]
    }
   ],
   "source": [
    "random.seed(555)\n",
    "tmp = sentence.split()\n",
    "random.shuffle(tmp)\n",
    "tmp_sent_2 = ' '.join(tmp)\n",
    "print(tmp_sent_2)\n",
    "print(get_ppl(bigram_model, tmp_sent_2))\n",
    "print(get_ppl(trigram_model, tmp_sent_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that perplexity gets higher, but not as high as the out-of-domain sentence. \n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we know if a word is OOV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV word!\n"
     ]
    }
   ],
   "source": [
    "random_word='wioruqoeruq4r'\n",
    "if random_word not in bigram_model:\n",
    "    print('OOV word!')\n",
    "else:\n",
    "    print('not OOV word!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one to two corpora to test the language models on. Feel free to use one of following or any other appropriate corpus that appeals to you,\n",
    "\n",
    "Billion word dataset: http://www.statmt.org/lm-benchmark/  \n",
    "Quaker historical corpus: https://www.woodbrooke.org.uk/resource-library/quaker-historical-corpus/  \n",
    "All of Shakespeare: http://norvig.com/ngrams/  \n",
    "IMDB: http://ai.stanford.edu/~amaas/data/sentiment/  \n",
    "SNLI test set and MultiNLI dev-set (only hypothesis sentences) in data folder.\n",
    "\n",
    "**Exercise 1**: Load the data and get the perpelxity.\n",
    "\n",
    "**Exercise 2**: How many OOV words are there?\n",
    "\n",
    "**Exercise 3**: Find the sentence with the highest and lowest perplexity.\n",
    "\n",
    "**Exercise 4**: Do you think vocabulary size affect the perplexity of language model? Explore this by removing some words from training corpus.\n",
    "\n",
    "**Exercise 5**: If you want to train a model on a larger dataset, follow the directions on the KenLM website, and see how this new model fares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
