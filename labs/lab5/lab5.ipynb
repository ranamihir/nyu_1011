{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 1011 Fall 2018 Lab 5\n",
    "# Intrinsic Evaluation of Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, make sure you've downloaded the following: \n",
    "1. [GloVe vectors](https://nlp.stanford.edu/projects/glove/): We'll use the 6B, 50D version so download glove.6B.zip (822MB) from the website (or `wget http://nlp.stanford.edu/data/glove.6B.zip` )\n",
    "2. [fastText vectors](https://fasttext.cc/docs/en/english-vectors.html): We'll use the 1M, 300D version (650MB) (`wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load a set of 50D word vectors from GloVe. `glove_home` below specifies the location of the unzipped file. `words_to_load` specifies how many word vectors we want to load. The words are saved in frequency order, so specifying 50,000 means that we only want to work with the 50,000 most frequent words from the source corpus. You can load up to 400,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(glove_home + 'glove.6B.50d.txt') as f:\n",
    "    loaded_embeddings = np.zeros((words_to_load, 50))\n",
    "    words = {}\n",
    "    idx2words = {}\n",
    "    ordered_words = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings[i, :] = np.asarray(s[1:])\n",
    "        words[s[0]] = i\n",
    "        idx2words[i] = s[0]\n",
    "        ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to look up a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.063054 -0.62636  -0.76417  -0.041484  0.56284   0.86432  -0.73734\n",
      " -0.70925  -0.073065 -0.74619  -0.34769   0.14402   1.4576    0.034688\n",
      "  0.11224   0.13854   0.10484   0.60207   0.021777 -0.21802   0.087613\n",
      " -1.4234    1.0361    0.1509    0.13608  -0.2971   -0.90828   0.34182\n",
      "  1.3367    0.16329   1.2374   -0.20113  -0.91532   1.4222   -0.1276\n",
      "  0.69443  -1.1782    1.2072    1.0524   -0.11957  -0.1275    0.41798\n",
      " -0.9232   -0.1312    1.2696    1.2318    0.30061  -0.18854   0.15899\n",
      "  0.0486  ]\n"
     ]
    }
   ],
   "source": [
    "# loaded_embeddings: original embedding matrix, dim = (words_to_load, 50)\n",
    "# words: a dictionary that maps word to its idx\n",
    "# idx2words: a dictionary that maps idx to word\n",
    "print(loaded_embeddings[words['potato']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Similarity Measure\n",
    "\n",
    "Implement the function dot_similarity that returns the same similarity as the cosine_similarity in sklearn for the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7964893661716317\n",
      "1.0\n",
      "0.8510908646017762\n",
      "1.0\n",
      "0.4897107000383957\n"
     ]
    }
   ],
   "source": [
    "def sklearn_cosine_similarity(vec_one, vec_two):\n",
    "    \"\"\"\n",
    "    Function that calculates the cosine similarity between two words\n",
    "    \"\"\"\n",
    "    return float(cosine_similarity(np.array([vec_one,vec_two]))[0,1])\n",
    "\n",
    "\n",
    "def handcraft_cosine_similarity(vec_one, vec_two):\n",
    "    \"\"\"\n",
    "    Function that calculates the cosine similarity between two words\n",
    "    \"\"\"\n",
    "    #TODO: fill in your code\n",
    "    return 1.0\n",
    "\n",
    "# Your handcraft_cosine_similarity should give (almost) same values as sklearn_cosine_similarity\n",
    "print(handcraft_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"bad\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"bad\"]]))\n",
    "\n",
    "print(handcraft_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"well\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"well\"]]))\n",
    "\n",
    "print(handcraft_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"fish\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings[words[\"good\"]], loaded_embeddings[words[\"fish\"]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Semantic Orientation Method\n",
    "\n",
    "The __semantic orientation__ method of [Turney and Littman 2003](http://doi.acm.org/10.1145/944012.944013) is a method for automatically scoring words along some single semantic dimension like sentiment. It works from a pair of small seed sets of words that represent two opposing points on that dimension.\n",
    "\n",
    "*Some code in this section was adapted from Stanford CS 224U*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_coefficient(candidate_word, loaded_embeddings):\n",
    "    # Here's a sample pair of seed sets:\n",
    "    seed_pos = ['table', 'chair', 'lamp', 'curtain', 'desk']\n",
    "    seed_neg = ['fish', 'bird', 'dog', 'cat', 'cow']\n",
    "    \n",
    "    # Let's look up the embeddings for these words.\n",
    "    seed_pos_indices = [words[seed] for seed in seed_pos]\n",
    "    seed_neg_indices = [words[seed] for seed in seed_neg]\n",
    "    seed_pos_mat = loaded_embeddings[seed_pos_indices]\n",
    "    seed_neg_mat = loaded_embeddings[seed_neg_indices]\n",
    "\n",
    "    # Scoring words along the axis\n",
    "    candidate = loaded_embeddings[words[candidate_word]]\n",
    "    pos_sim = np.sum([cosine_similarity(np.array([candidate,reference]))[0,1] for reference in seed_pos_mat])\n",
    "    neg_sim = np.sum([cosine_similarity(np.array([candidate,reference]))[0,1] for reference in seed_neg_mat])\n",
    "    return pos_sim - neg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2916448442212511"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "determine_coefficient('abhorrent', loaded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sort our vocabulary by its score along the axis. For now, we're only scoring frequent words, since this process can be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scored_words = [(word, determine_coefficient(word, loaded_embeddings)) for word in ordered_words[1:10000]]\n",
    "sorted_words = sorted(scored_words, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('panels', 2.0888931338753922),\n",
      "    ('desk', 2.031519353296948),\n",
      "    ('chairs', 1.9969309470439887),\n",
      "    ('chair', 1.9807613618158875),\n",
      "    ('slobodan', 1.9798640819000619),\n",
      "    ('ceiling', 1.9240190533444927),\n",
      "    ('doors', 1.9204824593800405),\n",
      "    ('rotating', 1.8855537456237359),\n",
      "    ('belgrade', 1.8764716706543156),\n",
      "    ('columns', 1.8563476943420172)]\n",
      "[   ('cow', -2.9569226842503795),\n",
      "    ('breeding', -2.9796908223187222),\n",
      "    ('breed', -2.985939176876923),\n",
      "    ('bird', -3.0657022888194758),\n",
      "    ('cats', -3.1399804835837362),\n",
      "    ('cattle', -3.1442049121562579),\n",
      "    ('whale', -3.1587269448292599),\n",
      "    ('shark', -3.2280199929166615),\n",
      "    ('sheep', -3.2573173088766909),\n",
      "    ('pigs', -3.3829592988198574)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(sorted_words[:10])\n",
    "pp.pprint(sorted_words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend a few minutes exploring possible seed sets for other semantic dimensions. What works? What doesn't? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Word Analogies\n",
    "\n",
    "\n",
    "The word analogy task consists of questions like, “a is to b as c is to ?” As mentioned in the GloVe paper, the answer to this problem is the word that gives the max cosine similarity for equation emb(b) − emb(a) + emb(c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('assaulting', 0.6012762284317517), ('bartender', 0.5673272156148943)]\n",
      "[('wealthy', 0.78508652798614), ('poor', 0.7807281621143065)]\n",
      "[('exclaimed', 0.5803865159891104), ('dejected', 0.580236670612162), ('heartbroken', 0.5751155810308969), ('grimly', 0.5561391408895854), ('bewildered', 0.5340116744760797)]\n",
      "['american', 'young', 'many', 'popular']\n",
      "['d.c.', 'york', 'angeles', 'hollywood']\n",
      "['female', 'adult', 'girls', 'women']\n",
      "['germany', 'german', 'europe']\n",
      "['softer', 'texture', 'lean', 'thicker']\n"
     ]
    }
   ],
   "source": [
    "def find_nearest_word(input_vec, k=5):\n",
    "    \"\"\"\n",
    "    Function that returns the top k words whose embedding has the smallest cosine distance to the input_vec\n",
    "    @param input_vec: embedding for a single word\n",
    "    @param k: top k neighbours to return\n",
    "    \"\"\"\n",
    "    #TODO: fill in your code\n",
    "    return None\n",
    "\n",
    "\n",
    "def word_analogy(word_a, word_b, word_c, k=5):\n",
    "    \"\"\"\n",
    "    Function that solves problem word_a to word_b = word_c to ?\n",
    "    @param word_a, word_b, word_c: string\n",
    "    @param k: top k candidates to return\n",
    "    \"\"\"\n",
    "    #TODO: fill in your code\n",
    "    return None\n",
    "\n",
    "\n",
    "# embedding algebra\n",
    "print(find_nearest_word(loaded_embeddings[words[\"student\"]] - loaded_embeddings[words[\"study\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings[words[\"working-class\"]] + loaded_embeddings[words[\"money\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings[words[\"drunk\"]] - loaded_embeddings[words[\"alcohol\"]], k=5))\n",
    "\n",
    "\n",
    "# Analogy\n",
    "print(word_analogy(\"china\", \"chinese\", \"america\"))\n",
    "print(word_analogy(\"china\", \"beijing\", \"america\"))\n",
    "print(word_analogy(\"king\", \"male\", \"queen\"))\n",
    "print(word_analogy(\"athens\", \"greece\", \"berlin\"))\n",
    "print(word_analogy(\"dark\", \"darker\", \"soft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Fast Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try loading Fast text vectors and analyse them in a similar way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the cosine similarity between fT vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn_cosine_similarity(loaded_embeddings_ft[words[\"good\"]], loaded_embeddings[words_ft[\"bad\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings_ft[words[\"good\"]], loaded_embeddings[words_ft[\"well\"]]))\n",
    "print(sklearn_cosine_similarity(loaded_embeddings_ft[words[\"good\"]], loaded_embeddings[words_ft[\"fish\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the semantic orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_words = [(word, determine_coefficient(word, loaded_embeddings_ft)) for word in ordered_words[1:10000]]\n",
    "sorted_words = sorted(scored_words, key=itemgetter(1), reverse=True)\n",
    "pp.pprint(sorted_words[:10])\n",
    "pp.pprint(sorted_words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(find_nearest_word(loaded_embeddings_ft[words[\"student\"]] - loaded_embeddings_ft[words[\"study\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings_ft[words[\"working-class\"]] + loaded_embeddings_ft[words[\"money\"]], k=2))\n",
    "print(find_nearest_word(loaded_embeddings_ft[words[\"drunk\"]] - loaded_embeddings_ft[words[\"alcohol\"]], k=5))\n",
    "\n",
    "\n",
    "# Analogy\n",
    "print(word_analogy(\"china\", \"chinese\", \"america\"))\n",
    "print(word_analogy(\"china\", \"beijing\", \"america\"))\n",
    "print(word_analogy(\"king\", \"male\", \"queen\"))\n",
    "print(word_analogy(\"athens\", \"greece\", \"berlin\"))\n",
    "print(word_analogy(\"dark\", \"darker\", \"soft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Visualize word vectors (HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: TSNE plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More questions to think about:\n",
    "- Can we analyse and quantify the difference in Glove and fastText vectors?\n",
    "- If we only care about the nearest neighbour in a fixed set, will the neighbour with smallest L2 distance be the same neighbour that gives the max cosine similarity?\n",
    "- Will we lose any information about embeddings if we normalize the embedding vectors? Why?\n",
    "- Is cosine distance (1/cosine similarity) a valid distance metrics? Why?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
