{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1.post2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensor basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a library for training deep neural networks, and much of it is based on the `Tensor`, an array type that is similar to NumPy arrays.\n",
    "\n",
    "Under the hood, PyTorch runs on compiled C, and if available, CUDA and cuDNN code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.Tensor([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)\n",
    "print(tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to convert between NumPy arrays PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A better alternative to torch.Tensor(arr)\n",
    "torch.from_numpy(np.arange(5)) # this is inplace, unlike torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code only works if you have PyTorch set up for a GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available and torch.has_cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(range(5))\n",
    "y = torch.Tensor(np.ones(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's similarly easy to move Tensors onto a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "z = x.cuda() + y.cuda()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "z = x.to(device) + y.to(device)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises\n",
    "\n",
    "* (Taken from DS-GA 1011, Fall 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Initialize random tensors A, B, C of size [2,3], [2,3], [3,3,2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9300, 0.8611, 0.8363],\n",
       "         [0.6444, 0.9312, 0.3732]], dtype=torch.float64),\n",
       " tensor([[0.6157, 0.4155, 0.9577],\n",
       "         [0.5743, 0.6710, 0.7813]], dtype=torch.float64),\n",
       " tensor([[[0.8075, 0.6783],\n",
       "          [0.0646, 0.3027],\n",
       "          [0.5471, 0.0125]],\n",
       " \n",
       "         [[0.4627, 0.4488],\n",
       "          [0.5564, 0.7505],\n",
       "          [0.9059, 0.1614]],\n",
       " \n",
       "         [[0.9634, 0.0282],\n",
       "          [0.7441, 0.4344],\n",
       "          [0.5153, 0.0386]]], dtype=torch.float64))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.from_numpy(np.random.random((2,3)))\n",
    "B = torch.from_numpy(np.random.random((2,3)))\n",
    "C = torch.from_numpy(np.random.random((3,3,2)))\n",
    "A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Fill tensor A with all 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 10., 10.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.fill_(10)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Fill tensor B with elements sampled from the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1180,  0.8018, -2.9553],\n",
       "        [-2.0034, -0.8522,  1.6490]], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.normal_()\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Point-wise multiply A with B, and put the result into tensor B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-11.1798,   8.0185, -29.5529],\n",
       "        [-20.0336,  -8.5219,  16.4896]], dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.mul_(A)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Print the mean and standard deviation of the elements of B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.4634, dtype=torch.float64), tensor(17.1716, dtype=torch.float64))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.mean(), B.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Fill tensor C with elements samples from the uniform distribution U(-1,1). Print the dimensions of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0033, -0.9362],\n",
      "         [-0.1627, -0.2617],\n",
      "         [ 0.5622,  0.6178]],\n",
      "\n",
      "        [[ 0.3278,  0.6383],\n",
      "         [-0.9355, -0.3624],\n",
      "         [-0.3369,  0.2846]],\n",
      "\n",
      "        [[ 0.5988, -0.9512],\n",
      "         [ 0.6017, -0.6343],\n",
      "         [-0.3978,  0.6128]]], dtype=torch.float64)\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "C.uniform_(-1,1)\n",
    "print(C)\n",
    "print(C.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Transpose the second and third dimension of tensor C, and put the result into tensor C itself (in-place). Print the dimensions of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.transpose_(1,2)\n",
    "C.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Show the contiguity property of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.is_contiguous(), B.is_contiguous(), C.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Print the second column of the third dimension of tensor C (note zero-indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9512, -0.6343,  0.6128], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Perform operation A+B+C (note the broadcasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -1.1765,  17.8558, -18.9907],\n",
       "         [-10.9698,   1.2165,  27.1074]],\n",
       "\n",
       "        [[ -0.8519,  17.0829, -19.8899],\n",
       "         [ -9.3953,   1.1158,  26.7741]],\n",
       "\n",
       "        [[ -0.5809,  18.6202, -19.9507],\n",
       "         [-10.9848,   0.8438,  27.1024]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A+B+C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd is a submodule in PyTorch that handles automatic differentiations and gradient computation. This allows you to simply a define model once, in a forward fashion, and the library handles the computation of all gradients in the computational graph.\n",
    "\n",
    "Here, we create 2 Tensors, but we want PyTorch to compute gradients with respect to $x$. By default, for arbitrary computations in PyTorch, no gradiens are computed (e.g for $y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "y = torch.arange(5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3150,  0.5678, -1.2118,  1.1134, -1.1142], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined $z = x \\cdot y$. Then\n",
    "\n",
    "$$\\frac{dz}{dx} = y$$\n",
    "\n",
    "Note `z.grad_fn`, which shows $z$ was computed, capturing its dependencies in the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.9723, grad_fn=<SumBackward0>)\n",
      "None\n",
      "<SumBackward0 object at 0x7f9f8e9e4f60>\n"
     ]
    }
   ],
   "source": [
    "z = (x * y).sum()\n",
    "print(z)\n",
    "print(z.grad)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, no gradients are computed yet. It is only when we call `z.backward()` that PyTorch computes the gradients, and backpropagates them to any node in the graph that required gradients (e.g. $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, $x$ now has gradients associated with it, but $y$ does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just this, we can compute a very rudimentary form of gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very silly case of gradient descent:\n",
    "learning_rate = 0.01\n",
    "x = torch.tensor([1000.], requires_grad=True)\n",
    "x_values = []\n",
    "for i in range(1000):\n",
    "    \n",
    "    # Our loss function is: We want x**2 to be small\n",
    "    loss = x ** 2\n",
    "    loss.backward()\n",
    "    \n",
    "    # Have to do something a little convoluted here to subtract the \n",
    "    #   gradient -- don't worry, we'll never do this again\n",
    "    x.data.sub_(x.grad.data * learning_rate)\n",
    "    \n",
    "    # Remember to zero-out the gradient! \n",
    "    # PyTorch doesn't do it automatically.\n",
    "    x.grad.data.set_(torch.Tensor([0]))\n",
    "    x_values.append(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9f8e9bc710>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, sometimes you want to run things *without* computing gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1000.], requires_grad=True)\n",
    "\n",
    "# With gradient computation:\n",
    "loss = x ** 2\n",
    "print(loss.requires_grad)\n",
    "\n",
    "\n",
    "# Without gradient computation:\n",
    "with torch.no_grad():\n",
    "    loss = x ** 2\n",
    "print(loss.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highly Recommend**: https://pytorch.org/docs/stable/autograd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
