{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "PROJECT_DIR = '/home/mihir/Desktop/GitHub/nyu/nyu_1011/homeworks/'\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "PLOTS_DIR = os.path.join(PROJECT_DIR, 'plots')\n",
    "NUM_VAL = 5000\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_SIZE = 64     # input batch size for training\n",
    "N_EPOCHS = 50       # number of epochs to train\n",
    "LR = 0.01           # learning rate\n",
    "VOCAB_SIZE = 10000  # max vocab size\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "EMB_DIM = 100       # size of embedding\n",
    "\n",
    "\n",
    "# Save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset='train'):\n",
    "    data_path = os.path.join(DATA_DIR, dataset)\n",
    "    data = []\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        target = 1 if sentiment == 'pos' else 0\n",
    "        data_target_path = os.path.join(data_path, sentiment)\n",
    "        for file in os.listdir(data_target_path):\n",
    "            file_path = os.path.join(data_target_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r') as file_text:\n",
    "                    text = file_text.readlines()[0].replace(')', ' ').replace('(', ' ')\n",
    "                    text = re.sub('<[^<]+?>', '', text)\n",
    "                    data.append([text, target])\n",
    "    data = pd.DataFrame(data, columns=['text', 'sentiment'])\n",
    "    data['text'] = data['text'].astype(str)\n",
    "    data['sentiment'] = data['sentiment'].astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train_data):\n",
    "    train_data.sample(frac=1, random_state=1337)\n",
    "    val_data = train_data[:NUM_VAL]\n",
    "    train_data = train_data[NUM_VAL:]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_val_datasets(force=False):\n",
    "    train_data_path = os.path.join(DATA_DIR, 'train.pkl')\n",
    "    val_data_path = os.path.join(DATA_DIR, 'val.pkl')\n",
    "    if not force and os.path.exists(train_data_path) and os.path.exists(val_data_path):\n",
    "        train_data = pickle.load(open(train_data_path, 'rb'))\n",
    "        val_data = pickle.load(open(val_data_path, 'rb'))\n",
    "    else:\n",
    "        train_data = load_dataset('train')\n",
    "        train_data, val_data = split_train_val(train_data)\n",
    "        pickle.dump(train_data, open(train_data_path, 'wb'))\n",
    "        pickle.dump(val_data, open(val_data_path, 'wb'))\n",
    "    return train_data.reset_index(drop=True), val_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(force=False):\n",
    "    test_data_path = os.path.join(DATA_DIR, 'test.pkl')\n",
    "    if not force and os.path.exists(test_data_path):\n",
    "        test_data = pickle.load(open(test_data_path, 'rb'))\n",
    "    else:\n",
    "        test_data = load_dataset('test')\n",
    "        pickle.dump(test_data, open(test_data_path, 'wb'))\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = load_train_val_datasets()\n",
    "test_data = load_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size is {}\".format(len(train_data)))\n",
    "print(\"Val dataset size is {}\".format(len(val_data)))\n",
    "print(\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text         Another fun, witty, frothy RKO musical with As...\n",
      "sentiment                                                    1\n",
      "Name: 6961, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Random sample from train dataset\n",
    "print(train_data.iloc[np.random.randint(0, len(train_data)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stopwords():\n",
    "    NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\"no\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "    stopwords = STOP_WORDS.copy()\n",
    "    for word in STOP_WORDS:\n",
    "        if word in NEGATE:\n",
    "            stopwords.remove(word)\n",
    "\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tokens, stopwords, punctuations):\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in tokens]\n",
    "    tokens = [unidecode(tok) for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer+tagger+parser+NER+word vectors, and punctuations and stopwords\n",
    "tokenizer = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner'])\n",
    "punctuations = string.punctuation\n",
    "stopwords = prepare_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data, tokenizer, stopwords, punctuations, dataset='train', force=False):\n",
    "    tokens_data_path = os.path.join(DATA_DIR, '{}_tokenized.pkl'.format(dataset))\n",
    "    all_train_tokens_path = os.path.join(DATA_DIR, 'all_train_tokens.pkl')\n",
    "    if not force and os.path.exists(tokens_data_path):\n",
    "        tokens_data = pickle.load(open(tokens_data_path, 'rb'))\n",
    "        if dataset == 'train':\n",
    "            all_train_tokens = pickle.load(open(all_train_tokens_path, 'rb'))\n",
    "            return tokens_data, all_train_tokens\n",
    "        return tokens_data\n",
    "    else:\n",
    "        parsed_data = tokenizer.pipe(data['text'], batch_size=512, n_threads=-1)\n",
    "        tokens_data = pd.Series(parsed_data).apply(clean_data, args=(stopwords, punctuations))\n",
    "        pickle.dump(tokens_data, open(tokens_data_path, 'wb'))\n",
    "        if dataset == 'train':\n",
    "            all_train_tokens = np.hstack(tokens_data)\n",
    "            pickle.dump(all_train_tokens, open(all_train_tokens_path, 'wb'))\n",
    "            return tokens_data, all_train_tokens\n",
    "    return tokens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data, tokenizer, stopwords, punctuations, dataset='train')\n",
    "val_data_tokens = tokenize_dataset(val_data, tokenizer, stopwords, punctuations, dataset='val')\n",
    "test_data_tokens = tokenize_dataset(test_data, tokenizer, stopwords, punctuations, dataset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [film, watch, high, school, spanish, class, fa...\n",
       "1    [know, absolutely, nothing, ireland, love, lef...\n",
       "2    [watch, star, josie, lawrence, knewfrom, line,...\n",
       "3    [example, film, not, good, receive, stand, rai...\n",
       "4    [horror, not, educational, film, genre, huh, t...\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['film', 'watch', 'high', ..., 'wrong', 'val', 'kilmer'],\n",
       "      dtype='<U74')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in train dataset = 2175805\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of tokens in train dataset = {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(all_train_tokens):\n",
    "    '''\n",
    "    Returns:\n",
    "    id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    '''\n",
    "    \n",
    "    token_counter = Counter(all_train_tokens)\n",
    "    vocabulary, count = zip(*token_counter.most_common(VOCAB_SIZE))\n",
    "    id2token = list(vocabulary)\n",
    "    token2id = dict(zip(vocabulary, range(2, 2+len(vocabulary)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocabulary(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 453 ; token somewhat\n",
      "Token somewhat; token id 453\n"
     ]
    }
   ],
   "source": [
    "# Check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = np.random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print(\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print(\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = tokens_data.apply(lambda tokens: [token2id[token] if token in token2id else UNK_IDX \\\n",
    "                                                     for token in tokens])\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of review tokens \n",
    "        @param target_list: list of review targets \n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered dataset[i] is called\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdbreviews_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                            pad_width=((0, MAX_SENTENCE_LENGTH-datum[1])),\n",
    "                            mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBReviewsDataset(train_data_indices, train_data['sentiment'])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdbreviews_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBReviewsDataset(val_data_indices, val_data['sentiment'])\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdbreviews_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBReviewsDataset(test_data_indices, test_data['sentiment'])\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdbreviews_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        \n",
    "        # Pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0], 1).expand_as(out).float()\n",
    "     \n",
    "        # Return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Criterion, and Optimizer\n",
    "model = BagOfWords(len(id2token), EMB_DIM).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(dataloader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    model.eval()\n",
    "    for data_batch, lengths_batch, labels_batch in dataloader:\n",
    "        data_batch, lengths_batch, labels_batch = data_batch.to(DEVICE), lengths_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "        outputs = nn.functional.softmax(model(data_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels_batch.size(0)\n",
    "        correct += predicted.eq(labels_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/50], Step: [1/313], Validation Accuracy: 83.68\n",
      "Epoch: [1/50], Step: [101/313], Validation Accuracy: 77.26\n",
      "Epoch: [1/50], Step: [201/313], Validation Accuracy: 78.9\n",
      "Epoch: [1/50], Step: [301/313], Validation Accuracy: 84.88\n",
      "Epoch: [2/50], Step: [1/313], Validation Accuracy: 84.12\n",
      "Epoch: [2/50], Step: [101/313], Validation Accuracy: 84.1\n",
      "Epoch: [2/50], Step: [201/313], Validation Accuracy: 83.66\n",
      "Epoch: [2/50], Step: [301/313], Validation Accuracy: 84.26\n",
      "Epoch: [3/50], Step: [1/313], Validation Accuracy: 83.2\n",
      "Epoch: [3/50], Step: [101/313], Validation Accuracy: 82.84\n",
      "Epoch: [3/50], Step: [201/313], Validation Accuracy: 84.34\n",
      "Epoch: [3/50], Step: [301/313], Validation Accuracy: 81.0\n",
      "Epoch: [4/50], Step: [1/313], Validation Accuracy: 84.04\n",
      "Epoch: [4/50], Step: [101/313], Validation Accuracy: 82.68\n",
      "Epoch: [4/50], Step: [201/313], Validation Accuracy: 79.36\n",
      "Epoch: [4/50], Step: [301/313], Validation Accuracy: 80.76\n",
      "Epoch: [5/50], Step: [1/313], Validation Accuracy: 81.88\n",
      "Epoch: [5/50], Step: [101/313], Validation Accuracy: 82.86\n",
      "Epoch: [5/50], Step: [201/313], Validation Accuracy: 78.64\n",
      "Epoch: [5/50], Step: [301/313], Validation Accuracy: 83.0\n",
      "Epoch: [6/50], Step: [1/313], Validation Accuracy: 80.38\n",
      "Epoch: [6/50], Step: [101/313], Validation Accuracy: 79.48\n",
      "Epoch: [6/50], Step: [201/313], Validation Accuracy: 84.32\n",
      "Epoch: [6/50], Step: [301/313], Validation Accuracy: 80.56\n",
      "Epoch: [7/50], Step: [1/313], Validation Accuracy: 83.96\n",
      "Epoch: [7/50], Step: [101/313], Validation Accuracy: 82.52\n",
      "Epoch: [7/50], Step: [201/313], Validation Accuracy: 80.06\n",
      "Epoch: [7/50], Step: [301/313], Validation Accuracy: 79.76\n",
      "Epoch: [8/50], Step: [1/313], Validation Accuracy: 81.32\n",
      "Epoch: [8/50], Step: [101/313], Validation Accuracy: 80.7\n",
      "Epoch: [8/50], Step: [201/313], Validation Accuracy: 80.56\n",
      "Epoch: [8/50], Step: [301/313], Validation Accuracy: 81.34\n",
      "Epoch: [9/50], Step: [1/313], Validation Accuracy: 80.22\n",
      "Epoch: [9/50], Step: [101/313], Validation Accuracy: 80.88\n",
      "Epoch: [9/50], Step: [201/313], Validation Accuracy: 78.54\n",
      "Epoch: [9/50], Step: [301/313], Validation Accuracy: 79.06\n",
      "Epoch: [10/50], Step: [1/313], Validation Accuracy: 82.72\n",
      "Epoch: [10/50], Step: [101/313], Validation Accuracy: 81.04\n",
      "Epoch: [10/50], Step: [201/313], Validation Accuracy: 80.92\n",
      "Epoch: [10/50], Step: [301/313], Validation Accuracy: 82.58\n",
      "Epoch: [11/50], Step: [1/313], Validation Accuracy: 79.76\n",
      "Epoch: [11/50], Step: [101/313], Validation Accuracy: 79.0\n",
      "Epoch: [11/50], Step: [201/313], Validation Accuracy: 79.16\n",
      "Epoch: [11/50], Step: [301/313], Validation Accuracy: 82.6\n",
      "Epoch: [12/50], Step: [1/313], Validation Accuracy: 78.56\n",
      "Epoch: [12/50], Step: [101/313], Validation Accuracy: 81.04\n",
      "Epoch: [12/50], Step: [201/313], Validation Accuracy: 80.22\n",
      "Epoch: [12/50], Step: [301/313], Validation Accuracy: 82.2\n",
      "Epoch: [13/50], Step: [1/313], Validation Accuracy: 79.36\n",
      "Epoch: [13/50], Step: [101/313], Validation Accuracy: 81.58\n",
      "Epoch: [13/50], Step: [201/313], Validation Accuracy: 79.46\n",
      "Epoch: [13/50], Step: [301/313], Validation Accuracy: 81.42\n",
      "Epoch: [14/50], Step: [1/313], Validation Accuracy: 77.3\n",
      "Epoch: [14/50], Step: [101/313], Validation Accuracy: 80.34\n",
      "Epoch: [14/50], Step: [201/313], Validation Accuracy: 79.46\n",
      "Epoch: [14/50], Step: [301/313], Validation Accuracy: 82.18\n",
      "Epoch: [15/50], Step: [1/313], Validation Accuracy: 81.12\n",
      "Epoch: [15/50], Step: [101/313], Validation Accuracy: 78.62\n",
      "Epoch: [15/50], Step: [201/313], Validation Accuracy: 80.58\n",
      "Epoch: [15/50], Step: [301/313], Validation Accuracy: 82.92\n",
      "Epoch: [16/50], Step: [1/313], Validation Accuracy: 79.4\n",
      "Epoch: [16/50], Step: [101/313], Validation Accuracy: 81.82\n",
      "Epoch: [16/50], Step: [201/313], Validation Accuracy: 79.98\n",
      "Epoch: [16/50], Step: [301/313], Validation Accuracy: 77.44\n",
      "Epoch: [17/50], Step: [1/313], Validation Accuracy: 80.22\n",
      "Epoch: [17/50], Step: [101/313], Validation Accuracy: 76.8\n",
      "Epoch: [17/50], Step: [201/313], Validation Accuracy: 79.88\n",
      "Epoch: [17/50], Step: [301/313], Validation Accuracy: 79.32\n",
      "Epoch: [18/50], Step: [1/313], Validation Accuracy: 79.82\n",
      "Epoch: [18/50], Step: [101/313], Validation Accuracy: 80.76\n",
      "Epoch: [18/50], Step: [201/313], Validation Accuracy: 79.36\n",
      "Epoch: [18/50], Step: [301/313], Validation Accuracy: 80.04\n",
      "Epoch: [19/50], Step: [1/313], Validation Accuracy: 80.64\n",
      "Epoch: [19/50], Step: [101/313], Validation Accuracy: 81.06\n",
      "Epoch: [19/50], Step: [201/313], Validation Accuracy: 79.08\n",
      "Epoch: [19/50], Step: [301/313], Validation Accuracy: 80.7\n",
      "Epoch: [20/50], Step: [1/313], Validation Accuracy: 80.66\n",
      "Epoch: [20/50], Step: [101/313], Validation Accuracy: 81.44\n",
      "Epoch: [20/50], Step: [201/313], Validation Accuracy: 80.12\n",
      "Epoch: [20/50], Step: [301/313], Validation Accuracy: 80.96\n",
      "Epoch: [21/50], Step: [1/313], Validation Accuracy: 80.98\n",
      "Epoch: [21/50], Step: [101/313], Validation Accuracy: 80.38\n",
      "Epoch: [21/50], Step: [201/313], Validation Accuracy: 80.1\n",
      "Epoch: [21/50], Step: [301/313], Validation Accuracy: 80.8\n",
      "Epoch: [22/50], Step: [1/313], Validation Accuracy: 80.94\n",
      "Epoch: [22/50], Step: [101/313], Validation Accuracy: 80.36\n",
      "Epoch: [22/50], Step: [201/313], Validation Accuracy: 80.24\n",
      "Epoch: [22/50], Step: [301/313], Validation Accuracy: 80.48\n",
      "Epoch: [23/50], Step: [1/313], Validation Accuracy: 80.62\n",
      "Epoch: [23/50], Step: [101/313], Validation Accuracy: 80.42\n",
      "Epoch: [23/50], Step: [201/313], Validation Accuracy: 80.5\n",
      "Epoch: [23/50], Step: [301/313], Validation Accuracy: 80.5\n",
      "Epoch: [24/50], Step: [1/313], Validation Accuracy: 80.48\n",
      "Epoch: [24/50], Step: [101/313], Validation Accuracy: 80.66\n",
      "Epoch: [24/50], Step: [201/313], Validation Accuracy: 80.46\n",
      "Epoch: [24/50], Step: [301/313], Validation Accuracy: 80.58\n",
      "Epoch: [25/50], Step: [1/313], Validation Accuracy: 80.4\n",
      "Epoch: [25/50], Step: [101/313], Validation Accuracy: 80.54\n",
      "Epoch: [25/50], Step: [201/313], Validation Accuracy: 80.72\n",
      "Epoch: [25/50], Step: [301/313], Validation Accuracy: 80.6\n",
      "Epoch: [26/50], Step: [1/313], Validation Accuracy: 80.5\n",
      "Epoch: [26/50], Step: [101/313], Validation Accuracy: 80.36\n",
      "Epoch: [26/50], Step: [201/313], Validation Accuracy: 80.6\n",
      "Epoch: [26/50], Step: [301/313], Validation Accuracy: 80.46\n",
      "Epoch: [27/50], Step: [1/313], Validation Accuracy: 80.48\n",
      "Epoch: [27/50], Step: [101/313], Validation Accuracy: 80.68\n",
      "Epoch: [27/50], Step: [201/313], Validation Accuracy: 80.68\n",
      "Epoch: [27/50], Step: [301/313], Validation Accuracy: 80.62\n",
      "Epoch: [28/50], Step: [1/313], Validation Accuracy: 80.56\n",
      "Epoch: [28/50], Step: [101/313], Validation Accuracy: 80.58\n",
      "Epoch: [28/50], Step: [201/313], Validation Accuracy: 80.62\n",
      "Epoch: [28/50], Step: [301/313], Validation Accuracy: 80.56\n",
      "Epoch: [29/50], Step: [1/313], Validation Accuracy: 80.6\n",
      "Epoch: [29/50], Step: [101/313], Validation Accuracy: 80.62\n",
      "Epoch: [29/50], Step: [201/313], Validation Accuracy: 80.46\n",
      "Epoch: [29/50], Step: [301/313], Validation Accuracy: 80.72\n",
      "Epoch: [30/50], Step: [1/313], Validation Accuracy: 80.74\n",
      "Epoch: [30/50], Step: [101/313], Validation Accuracy: 80.68\n",
      "Epoch: [30/50], Step: [201/313], Validation Accuracy: 80.62\n",
      "Epoch: [30/50], Step: [301/313], Validation Accuracy: 80.62\n",
      "Epoch: [31/50], Step: [1/313], Validation Accuracy: 80.68\n",
      "Epoch: [31/50], Step: [101/313], Validation Accuracy: 80.62\n",
      "Epoch: [31/50], Step: [201/313], Validation Accuracy: 80.6\n",
      "Epoch: [31/50], Step: [301/313], Validation Accuracy: 80.62\n",
      "Epoch: [32/50], Step: [1/313], Validation Accuracy: 80.64\n",
      "Epoch: [32/50], Step: [101/313], Validation Accuracy: 80.6\n",
      "Epoch: [32/50], Step: [201/313], Validation Accuracy: 80.66\n",
      "Epoch: [32/50], Step: [301/313], Validation Accuracy: 80.58\n",
      "Epoch: [33/50], Step: [1/313], Validation Accuracy: 80.56\n",
      "Epoch: [33/50], Step: [101/313], Validation Accuracy: 80.58\n",
      "Epoch: [33/50], Step: [201/313], Validation Accuracy: 80.68\n",
      "Epoch: [33/50], Step: [301/313], Validation Accuracy: 80.68\n",
      "Epoch: [34/50], Step: [1/313], Validation Accuracy: 80.68\n",
      "Epoch: [34/50], Step: [101/313], Validation Accuracy: 80.68\n",
      "Epoch: [34/50], Step: [201/313], Validation Accuracy: 80.64\n",
      "Epoch: [34/50], Step: [301/313], Validation Accuracy: 80.66\n",
      "Epoch: [35/50], Step: [1/313], Validation Accuracy: 80.64\n",
      "Epoch: [35/50], Step: [101/313], Validation Accuracy: 80.68\n",
      "Epoch: [35/50], Step: [201/313], Validation Accuracy: 80.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35/50], Step: [301/313], Validation Accuracy: 80.64\n",
      "Epoch: [36/50], Step: [1/313], Validation Accuracy: 80.64\n",
      "Epoch: [36/50], Step: [101/313], Validation Accuracy: 80.52\n",
      "Epoch: [36/50], Step: [201/313], Validation Accuracy: 80.54\n",
      "Epoch: [36/50], Step: [301/313], Validation Accuracy: 80.5\n",
      "Epoch: [37/50], Step: [1/313], Validation Accuracy: 80.52\n",
      "Epoch: [37/50], Step: [101/313], Validation Accuracy: 80.42\n",
      "Epoch: [37/50], Step: [201/313], Validation Accuracy: 80.48\n",
      "Epoch: [37/50], Step: [301/313], Validation Accuracy: 80.54\n",
      "Epoch: [38/50], Step: [1/313], Validation Accuracy: 80.56\n",
      "Epoch: [38/50], Step: [101/313], Validation Accuracy: 80.44\n",
      "Epoch: [38/50], Step: [201/313], Validation Accuracy: 80.5\n",
      "Epoch: [38/50], Step: [301/313], Validation Accuracy: 80.38\n",
      "Epoch: [39/50], Step: [1/313], Validation Accuracy: 80.38\n",
      "Epoch: [39/50], Step: [101/313], Validation Accuracy: 80.56\n",
      "Epoch: [39/50], Step: [201/313], Validation Accuracy: 80.56\n",
      "Epoch: [39/50], Step: [301/313], Validation Accuracy: 80.54\n",
      "Epoch: [40/50], Step: [1/313], Validation Accuracy: 80.46\n",
      "Epoch: [40/50], Step: [101/313], Validation Accuracy: 80.52\n",
      "Epoch: [40/50], Step: [201/313], Validation Accuracy: 80.52\n",
      "Epoch: [40/50], Step: [301/313], Validation Accuracy: 80.6\n",
      "Epoch: [41/50], Step: [1/313], Validation Accuracy: 80.62\n",
      "Epoch: [41/50], Step: [101/313], Validation Accuracy: 80.44\n",
      "Epoch: [41/50], Step: [201/313], Validation Accuracy: 80.54\n",
      "Epoch: [41/50], Step: [301/313], Validation Accuracy: 80.52\n",
      "Epoch: [42/50], Step: [1/313], Validation Accuracy: 80.48\n",
      "Epoch: [42/50], Step: [101/313], Validation Accuracy: 80.5\n",
      "Epoch: [42/50], Step: [201/313], Validation Accuracy: 80.46\n",
      "Epoch: [42/50], Step: [301/313], Validation Accuracy: 80.44\n",
      "Epoch: [43/50], Step: [1/313], Validation Accuracy: 80.44\n",
      "Epoch: [43/50], Step: [101/313], Validation Accuracy: 80.5\n",
      "Epoch: [43/50], Step: [201/313], Validation Accuracy: 80.46\n",
      "Epoch: [43/50], Step: [301/313], Validation Accuracy: 80.5\n",
      "Epoch: [44/50], Step: [1/313], Validation Accuracy: 80.48\n",
      "Epoch: [44/50], Step: [101/313], Validation Accuracy: 80.44\n",
      "Epoch: [44/50], Step: [201/313], Validation Accuracy: 80.54\n",
      "Epoch: [44/50], Step: [301/313], Validation Accuracy: 80.48\n",
      "Epoch: [45/50], Step: [1/313], Validation Accuracy: 80.54\n",
      "Epoch: [45/50], Step: [101/313], Validation Accuracy: 80.48\n",
      "Epoch: [45/50], Step: [201/313], Validation Accuracy: 80.48\n",
      "Epoch: [45/50], Step: [301/313], Validation Accuracy: 80.42\n",
      "Epoch: [46/50], Step: [1/313], Validation Accuracy: 80.4\n",
      "Epoch: [46/50], Step: [101/313], Validation Accuracy: 80.4\n",
      "Epoch: [46/50], Step: [201/313], Validation Accuracy: 80.46\n",
      "Epoch: [46/50], Step: [301/313], Validation Accuracy: 80.38\n",
      "Epoch: [47/50], Step: [1/313], Validation Accuracy: 80.5\n",
      "Epoch: [47/50], Step: [101/313], Validation Accuracy: 80.44\n",
      "Epoch: [47/50], Step: [201/313], Validation Accuracy: 80.46\n",
      "Epoch: [47/50], Step: [301/313], Validation Accuracy: 80.42\n",
      "Epoch: [48/50], Step: [1/313], Validation Accuracy: 80.48\n",
      "Epoch: [48/50], Step: [101/313], Validation Accuracy: 80.4\n",
      "Epoch: [48/50], Step: [201/313], Validation Accuracy: 80.38\n",
      "Epoch: [48/50], Step: [301/313], Validation Accuracy: 80.44\n",
      "Epoch: [49/50], Step: [1/313], Validation Accuracy: 80.54\n",
      "Epoch: [49/50], Step: [101/313], Validation Accuracy: 80.44\n",
      "Epoch: [49/50], Step: [201/313], Validation Accuracy: 80.48\n",
      "Epoch: [49/50], Step: [301/313], Validation Accuracy: 80.58\n",
      "Epoch: [50/50], Step: [1/313], Validation Accuracy: 80.58\n",
      "Epoch: [50/50], Step: [101/313], Validation Accuracy: 80.54\n",
      "Epoch: [50/50], Step: [201/313], Validation Accuracy: 80.52\n",
      "Epoch: [50/50], Step: [301/313], Validation Accuracy: 80.52\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    for batch_idx, (data_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "        data_batch, lengths_batch, labels_batch = data_batch.to(DEVICE), lengths_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, lengths_batch)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:    # validate every 100 iterations\n",
    "            val_accuracy = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Accuracy: {}'\\\n",
    "                  .format(epoch+1, N_EPOCHS, batch_idx+1, len(train_loader), val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 50 epochs:\n",
      "Val Accuracy: 80.54\n",
      "Test Accuracy: 79.688\n"
     ]
    }
   ],
   "source": [
    "print(\"After training for {} epochs:\".format(N_EPOCHS))\n",
    "print(\"Val Accuracy: {}\".format(test_model(val_loader, model)))\n",
    "print(\"Test Accuracy: {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
