{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "NUM_CLASSES = 3\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "NUM_DIRECTIONS = 2 if BIDIRECTIONAL else 1\n",
    "EMB_HIDDEN_SIZE, CLASS_HIDDEN_SIZE = 256, 512\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 3e-4\n",
    "N_EPOCHS = 10\n",
    "N_EPOCHS_FINETUNE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Training on SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/snli_train.tsv', sep='\\t')\n",
    "val_data = pd.read_csv('data/snli_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'train': train_data,\n",
    "    'val': val_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A young girl in a pink shirt sitting on a dock...</td>\n",
       "      <td>A young girl watching the sunset over the water .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman is smiling while the man next to her i...</td>\n",
       "      <td>Two people are next to each other .</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Across the river , you can see a large building .</td>\n",
       "      <td>The large building is full of apartments and t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a man in white shorts and a black shirt is par...</td>\n",
       "      <td>A man is riding a jetski on the ocean .</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Four black dogs run together on bright green g...</td>\n",
       "      <td>Four dogs are preparing to be launched into sp...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A young girl in a pink shirt sitting on a dock...   \n",
       "1  A woman is smiling while the man next to her i...   \n",
       "2  Across the river , you can see a large building .   \n",
       "3  a man in white shorts and a black shirt is par...   \n",
       "4  Four black dogs run together on bright green g...   \n",
       "\n",
       "                                           sentence2          label  \n",
       "0  A young girl watching the sunset over the water .        neutral  \n",
       "1                Two people are next to each other .     entailment  \n",
       "2  The large building is full of apartments and t...        neutral  \n",
       "3            A man is riding a jetski on the ocean .  contradiction  \n",
       "4  Four dogs are preparing to be launched into sp...  contradiction  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A young girl in a pink shirt sitting on a dock...</td>\n",
       "      <td>A young girl watching the sunset over the water .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman is smiling while the man next to her i...</td>\n",
       "      <td>Two people are next to each other .</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Across the river , you can see a large building .</td>\n",
       "      <td>The large building is full of apartments and t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a man in white shorts and a black shirt is par...</td>\n",
       "      <td>A man is riding a jetski on the ocean .</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Four black dogs run together on bright green g...</td>\n",
       "      <td>Four dogs are preparing to be launched into sp...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A young girl in a pink shirt sitting on a dock...   \n",
       "1  A woman is smiling while the man next to her i...   \n",
       "2  Across the river , you can see a large building .   \n",
       "3  a man in white shorts and a black shirt is par...   \n",
       "4  Four black dogs run together on bright green g...   \n",
       "\n",
       "                                           sentence2          label  \n",
       "0  A young girl watching the sunset over the water .        neutral  \n",
       "1                Two people are next to each other .     entailment  \n",
       "2  The large building is full of apartments and t...        neutral  \n",
       "3            A man is riding a jetski on the ocean .  contradiction  \n",
       "4  Four dogs are preparing to be launched into sp...  contradiction  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    data['label'] = data['label'].map({'contradiction': 0, 'neutral': 1, 'entailment': 2})\n",
    "    data['sentence1'] = data['sentence1'].str.split()\n",
    "    data['sentence2'] = data['sentence2'].str.split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(f_name, vocabulary):\n",
    "    f_in = io.open(f_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f_in.readline().split())\n",
    "    vectors = {}\n",
    "    for line in f_in:\n",
    "        tokens = line.strip().split(' ')\n",
    "        if tokens[0] in vocabulary:\n",
    "            vectors[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(train_data, vocab_size):\n",
    "    '''\n",
    "    Returns:\n",
    "    id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    '''\n",
    "    print('Building vocabulary... ', end='', flush=True)\n",
    "    all_tokens = []\n",
    "    for row in (train_data['sentence1']+train_data['sentence2']).iteritems():\n",
    "        all_tokens += row[1]\n",
    "    vocabulary, count = zip(*Counter(all_tokens).most_common(vocab_size))\n",
    "    print('Done.')\n",
    "    print('Loading vectors... ', end='', flush=True)\n",
    "    vectors = load_vectors('data/wiki-news-300d-1M.vec', vocabulary)\n",
    "    vocabulary = [word for word in vocabulary if word in vectors]\n",
    "    print('Done.')\n",
    "    id2token = list(vocabulary)\n",
    "    token2id = dict(zip(vocabulary, range(2, 2+len(vocabulary))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token, vectors\n",
    "\n",
    "def preprocess_data(data_dict, dataset, vocab_size=50000):\n",
    "    data = prepare_data(data_dict[dataset])\n",
    "    if dataset == 'train':\n",
    "        token2id, id2token, vectors = build_vocabulary(data, vocab_size)\n",
    "        return data, token2id, id2token, vectors\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, token2id, id2token, vectors = preprocess_data(data_dict, 'train', VOCAB_SIZE)\n",
    "# val_data = preprocess_data(data_dict, 'val')\n",
    "\n",
    "# pickle.dump(vectors, open('data/vectors.pkl', 'wb'))\n",
    "# pickle.dump(token2id, open('data/token2id.pkl', 'wb'))\n",
    "# pickle.dump(id2token, open('data/id2token.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(train_data)\n",
    "val_data = prepare_data(val_data)\n",
    "vectors = pickle.load(open('data/vectors.pkl', 'rb'))\n",
    "id2token = pickle.load(open('data/id2token.pkl', 'rb'))\n",
    "token2id = pickle.load(open('data/token2id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22059"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id: 14732; Token: burping\n",
      "Token: burping; Token id: 14732\n"
     ]
    }
   ],
   "source": [
    "# Check the dictionary by loading random token from it\n",
    "random_token_id = np.random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "print(\"Token id: {}; Token: {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print(\"Token: {}; Token id: {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_sentence_length(train_data, q=0.95):\n",
    "    max_sent1_len = train_data['sentence1'].str.len().quantile(q)\n",
    "    max_sent2_len = train_data['sentence2'].str.len().quantile(q)\n",
    "    return int(max(max_sent1_len, max_sent2_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = get_max_sentence_length(train_data)\n",
    "MAX_SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, token2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.x1, self.x2, self.y = data['sentence1'].values, data['sentence2'].values, data['label'].values\n",
    "        assert (len(self.x1) == len(self.x2) == len(self.y))\n",
    "        self.token2id = token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, row):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        label = self.y[row]\n",
    "        x1_word_idx, x2_word_idx = [], []\n",
    "        x1_mask, x2_mask = [], []\n",
    "        \n",
    "        for word in self.x1[row][:MAX_SENT_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                x1_word_idx.append(self.token2id[word])\n",
    "                x1_mask.append(0)\n",
    "            else:\n",
    "                x1_word_idx.append(UNK_IDX)\n",
    "                x1_mask.append(1)\n",
    "                \n",
    "        for word in self.x2[row][:MAX_SENT_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                x2_word_idx.append(self.token2id[word])\n",
    "                x2_mask.append(0)\n",
    "            else:\n",
    "                x2_word_idx.append(UNK_IDX)\n",
    "                x2_mask.append(1)\n",
    "        \n",
    "        x1_list = [x1_word_idx, x1_mask, len(x1_word_idx)]\n",
    "        x2_list = [x2_word_idx, x2_mask, len(x2_word_idx)]\n",
    "        return x1_list + x2_list + [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    x1_data, x2_data = [], []\n",
    "    x1_mask, x2_mask = [], []\n",
    "    x1_lengths, x2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        x1_lengths.append(datum[2])\n",
    "        x2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "        \n",
    "        # Padding\n",
    "        x1_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENT_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x1_data.append(x1_data_padded)\n",
    "        \n",
    "        x1_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENT_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x1_mask.append(x1_mask_padded)\n",
    "        \n",
    "        x2_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENT_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x2_data.append(x2_data_padded)\n",
    "        \n",
    "        x2_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENT_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        x2_mask.append(x2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(x1_lengths)[::-1]\n",
    "    \n",
    "    x1_data = np.array(x1_data)[ind_dec_order]\n",
    "    x2_data = np.array(x2_data)[ind_dec_order]\n",
    "    \n",
    "    x1_mask = np.array(x1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    x2_mask = np.array(x2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    \n",
    "    x1_lengths = np.array(x1_lengths)[ind_dec_order]\n",
    "    x2_lengths = np.array(x2_lengths)[ind_dec_order]\n",
    "    \n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    x1_list = [torch.from_numpy(x1_data), torch.from_numpy(x1_mask).float(), x1_lengths]\n",
    "    x2_list = [torch.from_numpy(x2_data), torch.from_numpy(x2_mask).float(), x2_lengths]\n",
    "        \n",
    "    return x1_list + x2_list + [torch.from_numpy(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train and validation dataloaders\n",
    "\n",
    "train_dataset = SNLIDataset(train_data, token2id)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          collate_fn=snli_collate_func,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset = SNLIDataset(val_data, token2id)\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        collate_fn=snli_collate_func,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, emb_weights, vocab_size, bidirectional=False):\n",
    "        '''\n",
    "        params:\n",
    "            hidden_size: hidden Size of layer in GRU\n",
    "            num_layers: number of layers in GRU\n",
    "            output_size: dimension of output\n",
    "            vocab_size: vocabulary size\n",
    "            bidirectional: use bidirectional GRU\n",
    "        '''\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=PAD_IDX)\n",
    "        self.gru = nn.GRU(300, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_weights))\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        true2sorted = sorted(range(len(lengths)), key=lambda x: -lengths[x])\n",
    "        sorted2true = sorted(range(len(lengths)), key=lambda x: true2sorted[x])\n",
    "        x = x[true2sorted]\n",
    "        mask = mask[true2sorted]\n",
    "        lengths = lengths[true2sorted]\n",
    "        \n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # reset hidden state\n",
    "        self.hidden = self._init_hidden(batch_size)\n",
    "        \n",
    "        # get embedding of words\n",
    "        embed = self.embedding(x)\n",
    "        \n",
    "        # mask out all embeddings other than <unk> token to freeze their weights\n",
    "        embed = mask*embed + (1-mask)*embed.clone().detach()\n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        \n",
    "        # forward prop though GRU\n",
    "        gru_out, self.hidden = self.gru(embed, self.hidden)\n",
    "        \n",
    "        # undo packing\n",
    "        gru_out, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out, batch_first=True)\n",
    "        \n",
    "        # (batch_size, seq_len, num_directions*hidden_size) -> (batch_size, seq_len, num_directions, hidden_size)\n",
    "        gru_out = gru_out.view(batch_size, -1, self.num_directions, self.hidden_size)\n",
    "        \n",
    "        # sum hidden activations of GRU across time\n",
    "        gru_out = torch.sum(gru_out, dim=1)\n",
    "        \n",
    "        # concat all directions along the hidden dimension\n",
    "        gru_out = torch.cat([gru_out[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "        \n",
    "        # get data back in original order of batches\n",
    "        gru_out = gru_out[sorted2true]\n",
    "        \n",
    "        return gru_out\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=3, num_directions=1, mode='cat'):\n",
    "        super(ClassificationNetwork, self).__init__()\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        # Fully connected and ReLU layers\n",
    "        if mode == 'cat':\n",
    "            self.fc1 = nn.Linear(2*input_size*num_directions, hidden_size)\n",
    "        elif mode in ['elementwise_mult', 'sum']:\n",
    "            self.fc1 = nn.Linear(input_size*num_directions, hidden_size)\n",
    "        else:\n",
    "            raise ValueError('Invalid arugment \"{}\" for mode!'.format(mode))\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, embedding_output1, embedding_output2):\n",
    "        if self.mode == 'cat':\n",
    "            input = torch.cat([embedding_output1, embedding_output2], dim=1)\n",
    "        elif self.mode == 'elementwise_mult':\n",
    "            input = embedding_output1 * embedding_output2\n",
    "        elif self.mode == 'sum':\n",
    "            input = embedding_output1 + embedding_output2\n",
    "        \n",
    "        input = input.view(input.size(0), -1) # Reshape input to batch_size x num_inputs\n",
    "        output = self.fc1(input)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.uniform_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights matrix\n",
    "def init_embedding_weights(vectors, token2id, id2token):\n",
    "    weights = np.zeros((len(token2id), 300))\n",
    "    for idx in range(2, len(vectors)):\n",
    "        weights[idx] = np.array(vectors[id2token[idx]])\n",
    "    np.random.seed(1337)\n",
    "    weights[1] = np.random.randn(300)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(embedding_network, classification_network, dataloader, criterion, optimizer, epoch):\n",
    "    embedding_network.train()\n",
    "    classification_network.train()\n",
    "    \n",
    "    loss_train = 0.\n",
    "    \n",
    "    for batch_idx, (x1, x1_mask, x1_lengths, x2, x2_mask, x2_lengths, y) in enumerate(dataloader):\n",
    "        x1, x1_mask, x2, x2_mask, y = x1.to(DEVICE), x1_mask.to(DEVICE), x2.to(DEVICE), x2_mask.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        embedding_network.train()\n",
    "        classification_network.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        embedding_output1 = embedding_network(x1, x1_mask, x1_lengths)\n",
    "        embedding_output2 = embedding_network(x2, x2_mask, x2_lengths)\n",
    "        classification_output = classification_network(embedding_output1, embedding_output2)\n",
    "        \n",
    "        # Compute loss, back-prop, and take step\n",
    "        loss = criterion(classification_output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accurately compute loss, because of different batch size\n",
    "        loss_train += loss.item() * len(x1) / len(dataloader.dataset)\n",
    "\n",
    "        if (batch_idx+1) % (len(dataloader.dataset)//(10*y.shape[0])) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1) * y.shape[0], len(dataloader.dataset),\n",
    "                100. * (batch_idx+1) / len(dataloader), loss.item()))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(embedding_network, classification_network, dataloader, criterion):\n",
    "    embedding_network.eval()\n",
    "    classification_network.eval()\n",
    "    \n",
    "    loss_test = 0.\n",
    "    y_ls = []\n",
    "    output_ls = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x1, x1_mask, x1_lengths, x2, x2_mask, x2_lengths, y) in enumerate(dataloader):\n",
    "            x1, x1_mask, x2, x2_mask, y = x1.to(DEVICE), x1_mask.to(DEVICE), x2.to(DEVICE), x2_mask.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            embedding_output1 = embedding_network(x1, x1_mask, x1_lengths)\n",
    "            embedding_output2 = embedding_network(x2, x2_mask, x2_lengths)\n",
    "            classification_output = classification_network(embedding_output1, embedding_output2)\n",
    "            \n",
    "            loss = criterion(classification_output, y)\n",
    "\n",
    "            # Accurately compute loss, because of different batch size\n",
    "            loss_test += loss.item() / len(dataloader.dataset)\n",
    "\n",
    "            output_ls.append(classification_output)\n",
    "            y_ls.append(y)\n",
    "    return loss_test, torch.cat(output_ls, dim=0), torch.cat(y_ls, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(embedding_network, classification_network, dataloader, criterion):\n",
    "    _, y_predicted, y_true = test(\n",
    "        embedding_network=embedding_network,\n",
    "        classification_network=classification_network,\n",
    "        dataloader=dataloader,\n",
    "        criterion=criterion\n",
    "    )\n",
    "\n",
    "    y_predicted = y_predicted.max(1)[1]\n",
    "    return 100*y_predicted.eq(y_true.data.view_as(y_predicted)).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_loader, val_loader, embedding_network, classification_network, \\\n",
    "                 criterion_train, criterion_test, optimizer, n_epochs):\n",
    "    train_loss_history, train_accuracy_history = [], []\n",
    "    val_loss_history, val_accuracy_history = [], []\n",
    "    stop_epoch = n_epochs\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        try:\n",
    "            train_loss = train(\n",
    "                embedding_network=embedding_network,\n",
    "                classification_network=classification_network,\n",
    "                criterion=criterion_train,\n",
    "                dataloader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch\n",
    "            )\n",
    "\n",
    "            val_loss, val_pred, val_true = test(\n",
    "                embedding_network=embedding_network,\n",
    "                classification_network=classification_network,\n",
    "                dataloader=val_loader,\n",
    "                criterion=criterion_test\n",
    "            )\n",
    "\n",
    "            accuracy_train = accuracy(embedding_network, classification_network, train_loader, criterion_test)\n",
    "            accuracy_val = accuracy(embedding_network, classification_network, val_loader, criterion_test)\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "            train_accuracy_history.append(accuracy_train)\n",
    "            val_accuracy_history.append(accuracy_val)\n",
    "\n",
    "            print('TRAIN Epoch: {}\\tAverage loss: {:.4f}, Accuracy: {:.0f}%'.format(epoch, train_loss, accuracy_train))\n",
    "            print('VAL   Epoch: {}\\tAverage loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(epoch, val_loss, accuracy_val))\n",
    "        except KeyboardInterrupt:\n",
    "            # Save the model checkpoints\n",
    "            print('Keyboard Interrupted!')\n",
    "            stop_epoch = epoch-1\n",
    "            break\n",
    "    \n",
    "    return embedding_network, classification_network, optimizer, train_loss_history, \\\n",
    "            val_loss_history, train_accuracy_history, val_accuracy_history, stop_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = init_embedding_weights(vectors, token2id, id2token)\n",
    "\n",
    "# Encoder and Classification Networks\n",
    "embedding_network_gru = GRUEncoder(hidden_size=EMB_HIDDEN_SIZE, num_layers=NUM_LAYERS, emb_weights=WEIGHTS, vocab_size=len(token2id), bidirectional=BIDIRECTIONAL).to(DEVICE)\n",
    "classification_network_gru = ClassificationNetwork(EMB_HIDDEN_SIZE, CLASS_HIDDEN_SIZE, NUM_CLASSES, NUM_DIRECTIONS).to(DEVICE)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion_train = nn.CrossEntropyLoss()\n",
    "criterion_test = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer_gru = torch.optim.Adam(list(embedding_network_gru.parameters())+list(classification_network_gru.parameters()), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.036453\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.020683\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.042392\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.119557\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.061677\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.025813\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 1.058093\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 0.989946\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 1.114789\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.054854\n",
      "TRAIN Epoch: 1\tAverage loss: 1.1065, Accuracy: 52%\n",
      "VAL   Epoch: 1\tAverage loss: 1.0007, Accuracy: 49%\n",
      "\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.858191\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.990322\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.922997\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.885996\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.944486\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 0.951894\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.912588\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 0.898441\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 1.039339\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 1.134440\n",
      "TRAIN Epoch: 2\tAverage loss: 0.9546, Accuracy: 58%\n",
      "VAL   Epoch: 2\tAverage loss: 0.9275, Accuracy: 56%\n",
      "\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.937780\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 1.004785\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.956013\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.802432\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.805488\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 0.975113\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.893720\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.878305\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 1.035426\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.926567\n",
      "TRAIN Epoch: 3\tAverage loss: 0.8951, Accuracy: 60%\n",
      "VAL   Epoch: 3\tAverage loss: 0.8737, Accuracy: 59%\n",
      "\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.714938\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.869316\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.902781\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.722499\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.772484\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.989548\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.986324\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.988268\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.858659\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.799307\n",
      "TRAIN Epoch: 4\tAverage loss: 0.8620, Accuracy: 63%\n",
      "VAL   Epoch: 4\tAverage loss: 0.8325, Accuracy: 63%\n",
      "\n",
      "Train Epoch: 5 [9984/100000 (10%)]\tLoss: 0.624798\n",
      "Train Epoch: 5 [19968/100000 (20%)]\tLoss: 0.951793\n",
      "Train Epoch: 5 [29952/100000 (30%)]\tLoss: 0.868173\n",
      "Train Epoch: 5 [39936/100000 (40%)]\tLoss: 0.685672\n",
      "Train Epoch: 5 [49920/100000 (50%)]\tLoss: 0.687507\n",
      "Train Epoch: 5 [59904/100000 (60%)]\tLoss: 0.981159\n",
      "Train Epoch: 5 [69888/100000 (70%)]\tLoss: 0.962358\n",
      "Train Epoch: 5 [79872/100000 (80%)]\tLoss: 0.879661\n",
      "Train Epoch: 5 [89856/100000 (90%)]\tLoss: 0.776237\n",
      "Train Epoch: 5 [99840/100000 (100%)]\tLoss: 0.916433\n",
      "TRAIN Epoch: 5\tAverage loss: 0.8307, Accuracy: 65%\n",
      "VAL   Epoch: 5\tAverage loss: 0.8180, Accuracy: 64%\n",
      "\n",
      "Train Epoch: 6 [9984/100000 (10%)]\tLoss: 0.819604\n",
      "Train Epoch: 6 [19968/100000 (20%)]\tLoss: 0.835531\n",
      "Train Epoch: 6 [29952/100000 (30%)]\tLoss: 0.842773\n",
      "Train Epoch: 6 [39936/100000 (40%)]\tLoss: 0.816828\n",
      "Train Epoch: 6 [49920/100000 (50%)]\tLoss: 0.771980\n",
      "Train Epoch: 6 [59904/100000 (60%)]\tLoss: 0.706370\n",
      "Train Epoch: 6 [69888/100000 (70%)]\tLoss: 0.698000\n",
      "Train Epoch: 6 [79872/100000 (80%)]\tLoss: 0.624918\n",
      "Train Epoch: 6 [89856/100000 (90%)]\tLoss: 1.060173\n",
      "Train Epoch: 6 [99840/100000 (100%)]\tLoss: 0.851154\n",
      "TRAIN Epoch: 6\tAverage loss: 0.8027, Accuracy: 67%\n",
      "VAL   Epoch: 6\tAverage loss: 0.7881, Accuracy: 67%\n",
      "\n",
      "Train Epoch: 7 [9984/100000 (10%)]\tLoss: 0.622669\n",
      "Train Epoch: 7 [19968/100000 (20%)]\tLoss: 0.836298\n",
      "Train Epoch: 7 [29952/100000 (30%)]\tLoss: 0.782525\n",
      "Train Epoch: 7 [39936/100000 (40%)]\tLoss: 0.965592\n",
      "Train Epoch: 7 [49920/100000 (50%)]\tLoss: 0.816038\n",
      "Train Epoch: 7 [59904/100000 (60%)]\tLoss: 0.747822\n",
      "Train Epoch: 7 [69888/100000 (70%)]\tLoss: 0.790101\n",
      "Train Epoch: 7 [79872/100000 (80%)]\tLoss: 0.867071\n",
      "Train Epoch: 7 [89856/100000 (90%)]\tLoss: 0.911335\n",
      "Train Epoch: 7 [99840/100000 (100%)]\tLoss: 0.961307\n",
      "TRAIN Epoch: 7\tAverage loss: 0.7801, Accuracy: 69%\n",
      "VAL   Epoch: 7\tAverage loss: 0.7569, Accuracy: 69%\n",
      "\n",
      "Train Epoch: 8 [9984/100000 (10%)]\tLoss: 0.575270\n",
      "Train Epoch: 8 [19968/100000 (20%)]\tLoss: 0.695887\n",
      "Train Epoch: 8 [29952/100000 (30%)]\tLoss: 0.732194\n",
      "Train Epoch: 8 [39936/100000 (40%)]\tLoss: 0.638108\n",
      "Train Epoch: 8 [49920/100000 (50%)]\tLoss: 0.848249\n",
      "Train Epoch: 8 [59904/100000 (60%)]\tLoss: 0.576744\n",
      "Train Epoch: 8 [69888/100000 (70%)]\tLoss: 0.781753\n",
      "Train Epoch: 8 [79872/100000 (80%)]\tLoss: 0.594771\n",
      "Train Epoch: 8 [89856/100000 (90%)]\tLoss: 0.483725\n",
      "Train Epoch: 8 [99840/100000 (100%)]\tLoss: 0.948738\n",
      "TRAIN Epoch: 8\tAverage loss: 0.7551, Accuracy: 70%\n",
      "VAL   Epoch: 8\tAverage loss: 0.7424, Accuracy: 69%\n",
      "\n",
      "Train Epoch: 9 [9984/100000 (10%)]\tLoss: 0.596983\n",
      "Train Epoch: 9 [19968/100000 (20%)]\tLoss: 0.722561\n",
      "Train Epoch: 9 [29952/100000 (30%)]\tLoss: 0.776175\n",
      "Train Epoch: 9 [39936/100000 (40%)]\tLoss: 0.569260\n",
      "Train Epoch: 9 [49920/100000 (50%)]\tLoss: 0.735894\n",
      "Train Epoch: 9 [59904/100000 (60%)]\tLoss: 0.935091\n",
      "Train Epoch: 9 [69888/100000 (70%)]\tLoss: 0.835087\n",
      "Train Epoch: 9 [79872/100000 (80%)]\tLoss: 0.628464\n",
      "Train Epoch: 9 [89856/100000 (90%)]\tLoss: 0.663585\n",
      "Train Epoch: 9 [99840/100000 (100%)]\tLoss: 0.758045\n",
      "TRAIN Epoch: 9\tAverage loss: 0.7360, Accuracy: 71%\n",
      "VAL   Epoch: 9\tAverage loss: 0.7232, Accuracy: 69%\n",
      "\n",
      "Train Epoch: 10 [9984/100000 (10%)]\tLoss: 0.496123\n",
      "Train Epoch: 10 [19968/100000 (20%)]\tLoss: 0.658408\n",
      "Train Epoch: 10 [29952/100000 (30%)]\tLoss: 0.746397\n",
      "Train Epoch: 10 [39936/100000 (40%)]\tLoss: 0.717854\n",
      "Train Epoch: 10 [49920/100000 (50%)]\tLoss: 0.731649\n",
      "Train Epoch: 10 [59904/100000 (60%)]\tLoss: 0.528325\n",
      "Train Epoch: 10 [69888/100000 (70%)]\tLoss: 0.801823\n",
      "Train Epoch: 10 [79872/100000 (80%)]\tLoss: 0.637013\n",
      "Train Epoch: 10 [89856/100000 (90%)]\tLoss: 0.798101\n",
      "Train Epoch: 10 [99840/100000 (100%)]\tLoss: 0.683638\n",
      "TRAIN Epoch: 10\tAverage loss: 0.7162, Accuracy: 72%\n",
      "VAL   Epoch: 10\tAverage loss: 0.7218, Accuracy: 71%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_network_gru, classification_network_gru, optimizer_gru, train_loss_history_gru, val_loss_history_gru, \\\n",
    "    train_accuracy_history_gru, val_accuracy_history_gru, stop_epoch_gru = \\\n",
    "    run_training(train_loader, val_loader, embedding_network_gru, classification_network_gru, \\\n",
    "                 criterion_train, criterion_test, optimizer_gru, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, emb_weights):\n",
    "\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(300, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_weights))\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        embed = self.embedding(x)\n",
    "        \n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = self.relu(hidden.contiguous().view(-1, hidden.size(-1)))\n",
    "        hidden = hidden.view(batch_size, seq_len, hidden.size(-1))\n",
    "        \n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = self.relu(hidden.contiguous().view(-1, hidden.size(-1)))\n",
    "        hidden = hidden.view(batch_size, seq_len, hidden.size(-1))\n",
    "        \n",
    "        out = torch.sum(hidden, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = init_embedding_weights(vectors, token2id, id2token)\n",
    "\n",
    "# Encoder and Classification Networks\n",
    "embedding_network_cnn = CNNEncoder(hidden_size=EMB_HIDDEN_SIZE, vocab_size=len(token2id), emb_weights=WEIGHTS).to(DEVICE)\n",
    "classification_network_cnn = ClassificationNetwork(EMB_HIDDEN_SIZE, CLASS_HIDDEN_SIZE, NUM_CLASSES, mode='elementwise_mult').to(DEVICE)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion_train = nn.CrossEntropyLoss()\n",
    "criterion_test = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer_cnn = torch.optim.Adam(list(embedding_network_cnn.parameters())+list(classification_network_cnn.parameters()), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 0.429160\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 0.156799\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 0.151030\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 0.179244\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 0.177438\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 0.099931\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.342025\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 0.152308\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.220606\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 0.302840\n",
      "TRAIN Epoch: 1\tAverage loss: 0.1750, Accuracy: 96%\n",
      "VAL   Epoch: 1\tAverage loss: 1.8184, Accuracy: 63%\n",
      "\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.292656\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.058935\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.086375\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.093388\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.068488\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 0.153034\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.244149\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 0.174420\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.120780\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.176798\n",
      "TRAIN Epoch: 2\tAverage loss: 0.1523, Accuracy: 97%\n",
      "VAL   Epoch: 2\tAverage loss: 2.1971, Accuracy: 64%\n",
      "\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.045414\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.125738\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.055084\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.093307\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.027998\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 0.186702\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.111134\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.049232\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.222084\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.108550\n",
      "TRAIN Epoch: 3\tAverage loss: 0.1305, Accuracy: 97%\n",
      "VAL   Epoch: 3\tAverage loss: 2.5702, Accuracy: 61%\n",
      "\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.051267\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.076472\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.131053\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.018962\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.043094\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.302234\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.180879\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.247129\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.104399\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.267540\n",
      "TRAIN Epoch: 4\tAverage loss: 0.1161, Accuracy: 97%\n",
      "VAL   Epoch: 4\tAverage loss: 2.4932, Accuracy: 64%\n",
      "\n",
      "Train Epoch: 5 [9984/100000 (10%)]\tLoss: 0.025145\n",
      "Train Epoch: 5 [19968/100000 (20%)]\tLoss: 0.021182\n",
      "Train Epoch: 5 [29952/100000 (30%)]\tLoss: 0.070901\n",
      "Train Epoch: 5 [39936/100000 (40%)]\tLoss: 0.046205\n",
      "Train Epoch: 5 [49920/100000 (50%)]\tLoss: 0.120047\n",
      "Train Epoch: 5 [59904/100000 (60%)]\tLoss: 0.116442\n",
      "Train Epoch: 5 [69888/100000 (70%)]\tLoss: 0.055206\n",
      "Train Epoch: 5 [79872/100000 (80%)]\tLoss: 0.025676\n",
      "Train Epoch: 5 [89856/100000 (90%)]\tLoss: 0.055167\n",
      "Train Epoch: 5 [99840/100000 (100%)]\tLoss: 0.015798\n",
      "TRAIN Epoch: 5\tAverage loss: 0.1028, Accuracy: 98%\n",
      "VAL   Epoch: 5\tAverage loss: 2.5770, Accuracy: 62%\n",
      "\n",
      "Train Epoch: 6 [9984/100000 (10%)]\tLoss: 0.015812\n",
      "Train Epoch: 6 [19968/100000 (20%)]\tLoss: 0.066883\n",
      "Train Epoch: 6 [29952/100000 (30%)]\tLoss: 0.018383\n",
      "Train Epoch: 6 [39936/100000 (40%)]\tLoss: 0.267618\n",
      "Train Epoch: 6 [49920/100000 (50%)]\tLoss: 0.509086\n",
      "Train Epoch: 6 [59904/100000 (60%)]\tLoss: 0.060522\n",
      "Train Epoch: 6 [69888/100000 (70%)]\tLoss: 0.040552\n",
      "Train Epoch: 6 [79872/100000 (80%)]\tLoss: 0.227660\n",
      "Train Epoch: 6 [89856/100000 (90%)]\tLoss: 0.126243\n",
      "Train Epoch: 6 [99840/100000 (100%)]\tLoss: 0.069680\n",
      "TRAIN Epoch: 6\tAverage loss: 0.0906, Accuracy: 98%\n",
      "VAL   Epoch: 6\tAverage loss: 3.2300, Accuracy: 62%\n",
      "\n",
      "Train Epoch: 7 [9984/100000 (10%)]\tLoss: 0.005830\n",
      "Train Epoch: 7 [19968/100000 (20%)]\tLoss: 0.025169\n",
      "Train Epoch: 7 [29952/100000 (30%)]\tLoss: 0.043536\n",
      "Train Epoch: 7 [39936/100000 (40%)]\tLoss: 0.017632\n",
      "Train Epoch: 7 [49920/100000 (50%)]\tLoss: 0.013604\n",
      "Train Epoch: 7 [59904/100000 (60%)]\tLoss: 0.059518\n",
      "Train Epoch: 7 [69888/100000 (70%)]\tLoss: 0.014315\n",
      "Train Epoch: 7 [79872/100000 (80%)]\tLoss: 0.040676\n",
      "Train Epoch: 7 [89856/100000 (90%)]\tLoss: 0.049324\n",
      "Train Epoch: 7 [99840/100000 (100%)]\tLoss: 0.072477\n",
      "TRAIN Epoch: 7\tAverage loss: 0.0820, Accuracy: 98%\n",
      "VAL   Epoch: 7\tAverage loss: 3.1961, Accuracy: 62%\n",
      "\n",
      "Train Epoch: 8 [9984/100000 (10%)]\tLoss: 0.091342\n",
      "Train Epoch: 8 [19968/100000 (20%)]\tLoss: 0.069081\n",
      "Train Epoch: 8 [29952/100000 (30%)]\tLoss: 0.012427\n",
      "Train Epoch: 8 [39936/100000 (40%)]\tLoss: 0.037786\n",
      "Train Epoch: 8 [49920/100000 (50%)]\tLoss: 0.107891\n",
      "Train Epoch: 8 [59904/100000 (60%)]\tLoss: 0.025611\n",
      "Train Epoch: 8 [69888/100000 (70%)]\tLoss: 0.012736\n",
      "Train Epoch: 8 [79872/100000 (80%)]\tLoss: 0.059959\n",
      "Train Epoch: 8 [89856/100000 (90%)]\tLoss: 0.045827\n",
      "Train Epoch: 8 [99840/100000 (100%)]\tLoss: 0.007827\n",
      "TRAIN Epoch: 8\tAverage loss: 0.0761, Accuracy: 99%\n",
      "VAL   Epoch: 8\tAverage loss: 3.0037, Accuracy: 63%\n",
      "\n",
      "Train Epoch: 9 [9984/100000 (10%)]\tLoss: 0.013537\n",
      "Train Epoch: 9 [19968/100000 (20%)]\tLoss: 0.006440\n",
      "Train Epoch: 9 [29952/100000 (30%)]\tLoss: 0.001201\n",
      "Train Epoch: 9 [39936/100000 (40%)]\tLoss: 0.016349\n",
      "Train Epoch: 9 [49920/100000 (50%)]\tLoss: 0.042038\n",
      "Train Epoch: 9 [59904/100000 (60%)]\tLoss: 0.070453\n",
      "Train Epoch: 9 [69888/100000 (70%)]\tLoss: 0.024453\n",
      "Train Epoch: 9 [79872/100000 (80%)]\tLoss: 0.018675\n",
      "Train Epoch: 9 [89856/100000 (90%)]\tLoss: 0.094960\n",
      "Train Epoch: 9 [99840/100000 (100%)]\tLoss: 0.053475\n",
      "TRAIN Epoch: 9\tAverage loss: 0.0722, Accuracy: 99%\n",
      "VAL   Epoch: 9\tAverage loss: 3.0514, Accuracy: 63%\n",
      "\n",
      "Train Epoch: 10 [9984/100000 (10%)]\tLoss: 0.000341\n",
      "Train Epoch: 10 [19968/100000 (20%)]\tLoss: 0.222886\n",
      "Train Epoch: 10 [29952/100000 (30%)]\tLoss: 0.007228\n",
      "Train Epoch: 10 [39936/100000 (40%)]\tLoss: 0.026123\n",
      "Train Epoch: 10 [49920/100000 (50%)]\tLoss: 0.119582\n",
      "Train Epoch: 10 [59904/100000 (60%)]\tLoss: 0.013689\n",
      "Train Epoch: 10 [69888/100000 (70%)]\tLoss: 0.034617\n",
      "Train Epoch: 10 [79872/100000 (80%)]\tLoss: 0.288158\n",
      "Train Epoch: 10 [89856/100000 (90%)]\tLoss: 0.018779\n",
      "Train Epoch: 10 [99840/100000 (100%)]\tLoss: 0.027351\n",
      "TRAIN Epoch: 10\tAverage loss: 0.0660, Accuracy: 99%\n",
      "VAL   Epoch: 10\tAverage loss: 3.4744, Accuracy: 64%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_network_cnn, classification_network_cnn, optimizer_cnn, train_loss_history_cnn, val_loss_history_cnn, \\\n",
    "    train_accuracy_history_cnn, val_accuracy_history_cnn, stop_epoch_cnn = \\\n",
    "    run_training(train_loader, val_loader, embedding_network_cnn, classification_network_cnn, \\\n",
    "                 criterion_train, criterion_test, optimizer_cnn, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Evaluating on MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_data = pd.read_csv('data/mnli_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Not entirely , ' I snapped , harsher than int...</td>\n",
       "      <td>I spoke more harshly than I wanted to .</td>\n",
       "      <td>entailment</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cook and then the next time it would be my tur...</td>\n",
       "      <td>I would cook and then the next turn would be h...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The disorder hardly seemed to exist before the...</td>\n",
       "      <td>The disorder did n't seem to be as common when...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Report and Order , in large part , adopts ...</td>\n",
       "      <td>The Report and Order ignores recommendations f...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IDPA 's OIG 's mission is to prevent , detect ...</td>\n",
       "      <td>IDPA 's OIG 's mission is clear and cares abou...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  'Not entirely , ' I snapped , harsher than int...   \n",
       "1  cook and then the next time it would be my tur...   \n",
       "2  The disorder hardly seemed to exist before the...   \n",
       "3  The Report and Order , in large part , adopts ...   \n",
       "4  IDPA 's OIG 's mission is to prevent , detect ...   \n",
       "\n",
       "                                           sentence2          label  \\\n",
       "0            I spoke more harshly than I wanted to .     entailment   \n",
       "1  I would cook and then the next turn would be h...  contradiction   \n",
       "2  The disorder did n't seem to be as common when...     entailment   \n",
       "3  The Report and Order ignores recommendations f...  contradiction   \n",
       "4  IDPA 's OIG 's mission is clear and cares abou...     entailment   \n",
       "\n",
       "        genre  \n",
       "0     fiction  \n",
       "1   telephone  \n",
       "2       slate  \n",
       "3  government  \n",
       "4  government  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_data = prepare_data(mnli_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Not, entirely, ,, ', I, snapped, ,, harsher,...</td>\n",
       "      <td>[I, spoke, more, harshly, than, I, wanted, to, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[cook, and, then, the, next, time, it, would, ...</td>\n",
       "      <td>[I, would, cook, and, then, the, next, turn, w...</td>\n",
       "      <td>0</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, disorder, hardly, seemed, to, exist, bef...</td>\n",
       "      <td>[The, disorder, did, n't, seem, to, be, as, co...</td>\n",
       "      <td>2</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Report, and, Order, ,, in, large, part, ...</td>\n",
       "      <td>[The, Report, and, Order, ignores, recommendat...</td>\n",
       "      <td>0</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[IDPA, 's, OIG, 's, mission, is, to, prevent, ...</td>\n",
       "      <td>[IDPA, 's, OIG, 's, mission, is, clear, and, c...</td>\n",
       "      <td>2</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  ['Not, entirely, ,, ', I, snapped, ,, harsher,...   \n",
       "1  [cook, and, then, the, next, time, it, would, ...   \n",
       "2  [The, disorder, hardly, seemed, to, exist, bef...   \n",
       "3  [The, Report, and, Order, ,, in, large, part, ...   \n",
       "4  [IDPA, 's, OIG, 's, mission, is, to, prevent, ...   \n",
       "\n",
       "                                           sentence2  label       genre  \n",
       "0  [I, spoke, more, harshly, than, I, wanted, to, .]      2     fiction  \n",
       "1  [I, would, cook, and, then, the, next, turn, w...      0   telephone  \n",
       "2  [The, disorder, did, n't, seem, to, be, as, co...      2       slate  \n",
       "3  [The, Report, and, Order, ignores, recommendat...      0  government  \n",
       "4  [IDPA, 's, OIG, 's, mission, is, clear, and, c...      2  government  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "government    1016\n",
       "telephone     1005\n",
       "slate         1002\n",
       "fiction        995\n",
       "travel         982\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_data['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MNLI dataloaders for each genre\n",
    "mnli_val_genre_dataloader_dict, mnli_val_accuracies = {}, {}\n",
    "for genre in mnli_val_data['genre'].unique():\n",
    "    genre_data = mnli_val_data[mnli_val_data['genre'] == genre]\n",
    "    genre_dataset = SNLIDataset(genre_data, token2id)\n",
    "    mnli_val_genre_dataloader_dict[genre] = DataLoader(dataset=genre_dataset,\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=snli_collate_func,\n",
    "                                                   shuffle=False)\n",
    "    mnli_val_accuracies[genre] = accuracy(embedding_network_cnn, classification_network_cnn,  \\\n",
    "                                 mnli_val_genre_dataloader_dict[genre], criterion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fiction</th>\n",
       "      <td>41.407034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telephone</th>\n",
       "      <td>38.308460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slate</th>\n",
       "      <td>38.223553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>38.779527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>40.325868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy\n",
       "fiction     41.407034\n",
       "telephone   38.308460\n",
       "slate       38.223553\n",
       "government  38.779527\n",
       "travel      40.325868"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_accuracies_df = pd.DataFrame.from_dict(mnli_val_accuracies, orient='index', columns=['accuracy'])\n",
    "mnli_val_accuracies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Fine-tuning on MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_data = pd.read_csv('data/mnli_train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and now that was in fifty one that 's forty ye...</td>\n",
       "      <td>It was already a problem forty years ago but n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jon could smell baked bread on the air and his...</td>\n",
       "      <td>Jon smelt food in the air and was hungry .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it will be like Italian basketball with the uh...</td>\n",
       "      <td>This type of Italian basketball is nothing lik...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>well i think that 's about uh that 's about co...</td>\n",
       "      <td>Sorry but we are not done just yet .</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good job tenure , that is -- because in yet an...</td>\n",
       "      <td>Dr. Quinn , Medicine Woman , was worked on by ...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  and now that was in fifty one that 's forty ye...   \n",
       "1  Jon could smell baked bread on the air and his...   \n",
       "2  it will be like Italian basketball with the uh...   \n",
       "3  well i think that 's about uh that 's about co...   \n",
       "4  Good job tenure , that is -- because in yet an...   \n",
       "\n",
       "                                           sentence2          label      genre  \n",
       "0  It was already a problem forty years ago but n...        neutral  telephone  \n",
       "1         Jon smelt food in the air and was hungry .        neutral    fiction  \n",
       "2  This type of Italian basketball is nothing lik...  contradiction  telephone  \n",
       "3               Sorry but we are not done just yet .  contradiction  telephone  \n",
       "4  Dr. Quinn , Medicine Woman , was worked on by ...     entailment      slate  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_data = prepare_data(mnli_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[and, now, that, was, in, fifty, one, that, 's...</td>\n",
       "      <td>[It, was, already, a, problem, forty, years, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Jon, could, smell, baked, bread, on, the, air...</td>\n",
       "      <td>[Jon, smelt, food, in, the, air, and, was, hun...</td>\n",
       "      <td>1</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[it, will, be, like, Italian, basketball, with...</td>\n",
       "      <td>[This, type, of, Italian, basketball, is, noth...</td>\n",
       "      <td>0</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[well, i, think, that, 's, about, uh, that, 's...</td>\n",
       "      <td>[Sorry, but, we, are, not, done, just, yet, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Good, job, tenure, ,, that, is, --, because, ...</td>\n",
       "      <td>[Dr., Quinn, ,, Medicine, Woman, ,, was, worke...</td>\n",
       "      <td>2</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  [and, now, that, was, in, fifty, one, that, 's...   \n",
       "1  [Jon, could, smell, baked, bread, on, the, air...   \n",
       "2  [it, will, be, like, Italian, basketball, with...   \n",
       "3  [well, i, think, that, 's, about, uh, that, 's...   \n",
       "4  [Good, job, tenure, ,, that, is, --, because, ...   \n",
       "\n",
       "                                           sentence2  label      genre  \n",
       "0  [It, was, already, a, problem, forty, years, a...      1  telephone  \n",
       "1  [Jon, smelt, food, in, the, air, and, was, hun...      1    fiction  \n",
       "2  [This, type, of, Italian, basketball, is, noth...      0  telephone  \n",
       "3     [Sorry, but, we, are, not, done, just, yet, .]      0  telephone  \n",
       "4  [Dr., Quinn, ,, Medicine, Woman, ,, was, worke...      2      slate  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "telephone     4270\n",
       "slate         4026\n",
       "travel        3985\n",
       "government    3883\n",
       "fiction       3836\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train_data['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_genre_dataloader_dict = {}\n",
    "for genre in mnli_train_data['genre'].unique():\n",
    "    genre_data = mnli_train_data[mnli_train_data['genre'] == genre]\n",
    "    genre_dataset = SNLIDataset(genre_data, token2id)\n",
    "    mnli_train_genre_dataloader_dict[genre] = DataLoader(dataset=genre_dataset,\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         collate_fn=snli_collate_func,\n",
    "                                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dicts = {\n",
    "    'embedding_network': embedding_network_cnn.state_dict(),\n",
    "    'classification_network': classification_network_cnn.state_dict()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(embedding_network, classification_network, original_state_dicts, n_epochs):\n",
    "    results = {}\n",
    "    for genre in mnli_train_data['genre'].unique():\n",
    "        print('-'*50)\n",
    "        print('\\nGENRE: {}'.format(genre))\n",
    "        \n",
    "        # Get dataloaders for genre\n",
    "        train_loader_genre = mnli_train_genre_dataloader_dict[genre]\n",
    "        val_loader_genre = mnli_val_genre_dataloader_dict[genre]\n",
    "        \n",
    "        # Load initial (trained on SNLI) network state dicts\n",
    "        embedding_network.load_state_dict(original_state_dicts['embedding_network'])\n",
    "        classification_network.load_state_dict(original_state_dicts['classification_network'])\n",
    "        \n",
    "        # Criterion and Optimizer\n",
    "        criterion_train = nn.CrossEntropyLoss()\n",
    "        criterion_test = nn.CrossEntropyLoss(reduction='sum')\n",
    "        optimizer = torch.optim.Adam(list(embedding_network.parameters())+list(classification_network.parameters()), lr=LR)\n",
    "        \n",
    "        # Run training\n",
    "        embedding_network, classification_network, optimizer, train_loss_history, val_loss_history, \\\n",
    "            train_accuracy_history, val_accuracy_history, stop_epoch = \\\n",
    "            run_training(train_loader_genre, val_loader_genre, embedding_network, classification_network, \\\n",
    "                         criterion_train, criterion_test, optimizer, n_epochs)\n",
    "        print('-'*50)\n",
    "        \n",
    "        results[genre] = [embedding_network.state_dict(), classification_network.state_dict(), \\\n",
    "                          optimizer.state_dict(), train_loss_history, val_loss_history, \\\n",
    "                          train_accuracy_history, val_accuracy_history]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "GENRE: telephone\n",
      "Train Epoch: 1 [416/4270 (10%)]\tLoss: 1.199863\n",
      "Train Epoch: 1 [832/4270 (19%)]\tLoss: 1.216303\n",
      "Train Epoch: 1 [1248/4270 (29%)]\tLoss: 1.238578\n",
      "Train Epoch: 1 [1664/4270 (39%)]\tLoss: 1.014118\n",
      "Train Epoch: 1 [2080/4270 (49%)]\tLoss: 1.040132\n",
      "Train Epoch: 1 [2496/4270 (58%)]\tLoss: 0.965058\n",
      "Train Epoch: 1 [2912/4270 (68%)]\tLoss: 1.237062\n",
      "Train Epoch: 1 [3328/4270 (78%)]\tLoss: 1.110157\n",
      "Train Epoch: 1 [3744/4270 (87%)]\tLoss: 1.053408\n",
      "Train Epoch: 1 [4160/4270 (97%)]\tLoss: 1.024140\n",
      "TRAIN Epoch: 1\tAverage loss: 1.0922, Accuracy: 52%\n",
      "VAL   Epoch: 1\tAverage loss: 1.1092, Accuracy: 37%\n",
      "\n",
      "Train Epoch: 2 [416/4270 (10%)]\tLoss: 1.039150\n",
      "Train Epoch: 2 [832/4270 (19%)]\tLoss: 0.968401\n",
      "Train Epoch: 2 [1248/4270 (29%)]\tLoss: 0.960285\n",
      "Train Epoch: 2 [1664/4270 (39%)]\tLoss: 0.796023\n",
      "Train Epoch: 2 [2080/4270 (49%)]\tLoss: 0.825453\n",
      "Train Epoch: 2 [2496/4270 (58%)]\tLoss: 0.877364\n",
      "Train Epoch: 2 [2912/4270 (68%)]\tLoss: 1.070515\n",
      "Train Epoch: 2 [3328/4270 (78%)]\tLoss: 0.772411\n",
      "Train Epoch: 2 [3744/4270 (87%)]\tLoss: 1.047540\n",
      "Train Epoch: 2 [4160/4270 (97%)]\tLoss: 0.852374\n",
      "TRAIN Epoch: 2\tAverage loss: 0.9286, Accuracy: 61%\n",
      "VAL   Epoch: 2\tAverage loss: 1.2006, Accuracy: 42%\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "GENRE: fiction\n",
      "Train Epoch: 1 [352/3836 (9%)]\tLoss: 0.708118\n",
      "Train Epoch: 1 [704/3836 (18%)]\tLoss: 0.342261\n",
      "Train Epoch: 1 [1056/3836 (28%)]\tLoss: 0.324127\n",
      "Train Epoch: 1 [1408/3836 (37%)]\tLoss: 0.101853\n",
      "Train Epoch: 1 [1760/3836 (46%)]\tLoss: 0.253844\n",
      "Train Epoch: 1 [2112/3836 (55%)]\tLoss: 0.197037\n",
      "Train Epoch: 1 [2464/3836 (64%)]\tLoss: 0.385792\n",
      "Train Epoch: 1 [2816/3836 (73%)]\tLoss: 0.229755\n",
      "Train Epoch: 1 [3168/3836 (82%)]\tLoss: 0.124146\n",
      "Train Epoch: 1 [3520/3836 (92%)]\tLoss: 0.254406\n",
      "TRAIN Epoch: 1\tAverage loss: 0.3123, Accuracy: 99%\n",
      "VAL   Epoch: 1\tAverage loss: 2.8136, Accuracy: 43%\n",
      "\n",
      "Train Epoch: 2 [352/3836 (9%)]\tLoss: 0.060619\n",
      "Train Epoch: 2 [704/3836 (18%)]\tLoss: 0.112717\n",
      "Train Epoch: 2 [1056/3836 (28%)]\tLoss: 0.006839\n",
      "Train Epoch: 2 [1408/3836 (37%)]\tLoss: 0.026933\n",
      "Train Epoch: 2 [1760/3836 (46%)]\tLoss: 0.097937\n",
      "Train Epoch: 2 [2112/3836 (55%)]\tLoss: 0.017401\n",
      "Train Epoch: 2 [2464/3836 (64%)]\tLoss: 0.062113\n",
      "Train Epoch: 2 [2816/3836 (73%)]\tLoss: 0.057097\n",
      "Train Epoch: 2 [3168/3836 (82%)]\tLoss: 0.032965\n",
      "Train Epoch: 2 [3520/3836 (92%)]\tLoss: 0.012545\n",
      "TRAIN Epoch: 2\tAverage loss: 0.0516, Accuracy: 100%\n",
      "VAL   Epoch: 2\tAverage loss: 4.0471, Accuracy: 43%\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "GENRE: slate\n",
      "Train Epoch: 1 [384/4026 (10%)]\tLoss: 0.024264\n",
      "Train Epoch: 1 [768/4026 (19%)]\tLoss: 0.229060\n",
      "Train Epoch: 1 [1152/4026 (29%)]\tLoss: 0.188593\n",
      "Train Epoch: 1 [1536/4026 (38%)]\tLoss: 0.011298\n",
      "Train Epoch: 1 [1920/4026 (48%)]\tLoss: 0.074939\n",
      "Train Epoch: 1 [2304/4026 (57%)]\tLoss: 0.151387\n",
      "Train Epoch: 1 [2688/4026 (67%)]\tLoss: 0.490831\n",
      "Train Epoch: 1 [3072/4026 (76%)]\tLoss: 0.233123\n",
      "Train Epoch: 1 [3456/4026 (86%)]\tLoss: 0.093795\n",
      "Train Epoch: 1 [3840/4026 (95%)]\tLoss: 0.238137\n",
      "TRAIN Epoch: 1\tAverage loss: 0.2031, Accuracy: 99%\n",
      "VAL   Epoch: 1\tAverage loss: 4.0558, Accuracy: 40%\n",
      "\n",
      "Train Epoch: 2 [384/4026 (10%)]\tLoss: 0.050499\n",
      "Train Epoch: 2 [768/4026 (19%)]\tLoss: 0.004743\n",
      "Train Epoch: 2 [1152/4026 (29%)]\tLoss: 0.024949\n",
      "Train Epoch: 2 [1536/4026 (38%)]\tLoss: 0.043287\n",
      "Train Epoch: 2 [1920/4026 (48%)]\tLoss: 0.010236\n",
      "Train Epoch: 2 [2304/4026 (57%)]\tLoss: 0.046071\n",
      "Train Epoch: 2 [2688/4026 (67%)]\tLoss: 0.083600\n",
      "Train Epoch: 2 [3072/4026 (76%)]\tLoss: 0.163126\n",
      "Train Epoch: 2 [3456/4026 (86%)]\tLoss: 0.018820\n",
      "Train Epoch: 2 [3840/4026 (95%)]\tLoss: 0.016579\n",
      "TRAIN Epoch: 2\tAverage loss: 0.0328, Accuracy: 100%\n",
      "VAL   Epoch: 2\tAverage loss: 4.9896, Accuracy: 40%\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "GENRE: government\n",
      "Train Epoch: 1 [384/3883 (10%)]\tLoss: 0.043404\n",
      "Train Epoch: 1 [768/3883 (20%)]\tLoss: 0.036283\n",
      "Train Epoch: 1 [1152/3883 (30%)]\tLoss: 0.102087\n",
      "Train Epoch: 1 [1536/3883 (39%)]\tLoss: 0.073744\n",
      "Train Epoch: 1 [1920/3883 (49%)]\tLoss: 0.178044\n",
      "Train Epoch: 1 [2304/3883 (59%)]\tLoss: 0.135195\n",
      "Train Epoch: 1 [2688/3883 (69%)]\tLoss: 0.030082\n",
      "Train Epoch: 1 [3072/3883 (79%)]\tLoss: 0.028782\n",
      "Train Epoch: 1 [3456/3883 (89%)]\tLoss: 0.029684\n",
      "Train Epoch: 1 [3840/3883 (98%)]\tLoss: 0.016723\n",
      "TRAIN Epoch: 1\tAverage loss: 0.1190, Accuracy: 99%\n",
      "VAL   Epoch: 1\tAverage loss: 4.6112, Accuracy: 43%\n",
      "\n",
      "Train Epoch: 2 [384/3883 (10%)]\tLoss: 0.005815\n",
      "Train Epoch: 2 [768/3883 (20%)]\tLoss: 0.006044\n",
      "Train Epoch: 2 [1152/3883 (30%)]\tLoss: 0.007513\n",
      "Train Epoch: 2 [1536/3883 (39%)]\tLoss: 0.050660\n",
      "Train Epoch: 2 [1920/3883 (49%)]\tLoss: 0.072376\n",
      "Train Epoch: 2 [2304/3883 (59%)]\tLoss: 0.003315\n",
      "Train Epoch: 2 [2688/3883 (69%)]\tLoss: 0.135556\n",
      "Train Epoch: 2 [3072/3883 (79%)]\tLoss: 0.005873\n",
      "Train Epoch: 2 [3456/3883 (89%)]\tLoss: 0.043922\n",
      "Train Epoch: 2 [3840/3883 (98%)]\tLoss: 0.063875\n",
      "TRAIN Epoch: 2\tAverage loss: 0.0418, Accuracy: 99%\n",
      "VAL   Epoch: 2\tAverage loss: 5.2383, Accuracy: 42%\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "GENRE: travel\n",
      "Train Epoch: 1 [384/3985 (10%)]\tLoss: 2.407526\n",
      "Train Epoch: 1 [768/3985 (19%)]\tLoss: 1.464441\n",
      "Train Epoch: 1 [1152/3985 (29%)]\tLoss: 1.201463\n",
      "Train Epoch: 1 [1536/3985 (38%)]\tLoss: 0.966487\n",
      "Train Epoch: 1 [1920/3985 (48%)]\tLoss: 1.099404\n",
      "Train Epoch: 1 [2304/3985 (58%)]\tLoss: 1.265849\n",
      "Train Epoch: 1 [2688/3985 (67%)]\tLoss: 1.076123\n",
      "Train Epoch: 1 [3072/3985 (77%)]\tLoss: 0.955334\n",
      "Train Epoch: 1 [3456/3985 (86%)]\tLoss: 1.132877\n",
      "Train Epoch: 1 [3840/3985 (96%)]\tLoss: 1.442927\n",
      "TRAIN Epoch: 1\tAverage loss: 1.4307, Accuracy: 53%\n",
      "VAL   Epoch: 1\tAverage loss: 1.0760, Accuracy: 43%\n",
      "\n",
      "Train Epoch: 2 [384/3985 (10%)]\tLoss: 1.105990\n",
      "Train Epoch: 2 [768/3985 (19%)]\tLoss: 0.880197\n",
      "Train Epoch: 2 [1152/3985 (29%)]\tLoss: 0.904213\n",
      "Train Epoch: 2 [1536/3985 (38%)]\tLoss: 0.944238\n",
      "Train Epoch: 2 [1920/3985 (48%)]\tLoss: 0.849140\n",
      "Train Epoch: 2 [2304/3985 (58%)]\tLoss: 0.863816\n",
      "Train Epoch: 2 [2688/3985 (67%)]\tLoss: 0.968188\n",
      "Train Epoch: 2 [3072/3985 (77%)]\tLoss: 0.874301\n",
      "Train Epoch: 2 [3456/3985 (86%)]\tLoss: 0.915079\n",
      "Train Epoch: 2 [3840/3985 (96%)]\tLoss: 0.939944\n",
      "TRAIN Epoch: 2\tAverage loss: 0.9420, Accuracy: 62%\n",
      "VAL   Epoch: 2\tAverage loss: 1.1394, Accuracy: 46%\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = fine_tune(embedding_network_cnn, classification_network_cnn, original_state_dicts, N_EPOCHS_FINETUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:capstone_project]",
   "language": "python",
   "name": "conda-env-capstone_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
