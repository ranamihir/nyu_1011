{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import io\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(1337)\n",
    "\n",
    "DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = 50000\n",
    "NUM_CLASSES = 3\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "NUM_DIRECTIONS = 2 if BIDIRECTIONAL else 1\n",
    "EMB_HIDDEN_SIZE, CLASS_HODDEN_SIZE = 256, 512\n",
    "\n",
    "LR = 3e-4\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/snli_train.tsv', sep='\\t')\n",
    "val_data = pd.read_csv('data/snli_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'train': train_data,\n",
    "    'val': val_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A young girl in a pink shirt sitting on a dock...</td>\n",
       "      <td>A young girl watching the sunset over the water .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman is smiling while the man next to her i...</td>\n",
       "      <td>Two people are next to each other .</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Across the river , you can see a large building .</td>\n",
       "      <td>The large building is full of apartments and t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a man in white shorts and a black shirt is par...</td>\n",
       "      <td>A man is riding a jetski on the ocean .</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Four black dogs run together on bright green g...</td>\n",
       "      <td>Four dogs are preparing to be launched into sp...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A young girl in a pink shirt sitting on a dock...   \n",
       "1  A woman is smiling while the man next to her i...   \n",
       "2  Across the river , you can see a large building .   \n",
       "3  a man in white shorts and a black shirt is par...   \n",
       "4  Four black dogs run together on bright green g...   \n",
       "\n",
       "                                           sentence2          label  \n",
       "0  A young girl watching the sunset over the water .        neutral  \n",
       "1                Two people are next to each other .     entailment  \n",
       "2  The large building is full of apartments and t...        neutral  \n",
       "3            A man is riding a jetski on the ocean .  contradiction  \n",
       "4  Four dogs are preparing to be launched into sp...  contradiction  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A young girl in a pink shirt sitting on a dock...</td>\n",
       "      <td>A young girl watching the sunset over the water .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman is smiling while the man next to her i...</td>\n",
       "      <td>Two people are next to each other .</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Across the river , you can see a large building .</td>\n",
       "      <td>The large building is full of apartments and t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a man in white shorts and a black shirt is par...</td>\n",
       "      <td>A man is riding a jetski on the ocean .</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Four black dogs run together on bright green g...</td>\n",
       "      <td>Four dogs are preparing to be launched into sp...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A young girl in a pink shirt sitting on a dock...   \n",
       "1  A woman is smiling while the man next to her i...   \n",
       "2  Across the river , you can see a large building .   \n",
       "3  a man in white shorts and a black shirt is par...   \n",
       "4  Four black dogs run together on bright green g...   \n",
       "\n",
       "                                           sentence2          label  \n",
       "0  A young girl watching the sunset over the water .        neutral  \n",
       "1                Two people are next to each other .     entailment  \n",
       "2  The large building is full of apartments and t...        neutral  \n",
       "3            A man is riding a jetski on the ocean .  contradiction  \n",
       "4  Four dogs are preparing to be launched into sp...  contradiction  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    data['label'] = data['label'].map({'contradiction': 0, 'neutral': 1, 'entailment': 2})\n",
    "    data['sentence1'] = data['sentence1'].str.split()\n",
    "    data['sentence2'] = data['sentence2'].str.split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(f_name, vocabulary):\n",
    "    f_in = io.open(f_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, f_in.readline().split())\n",
    "    vectors = {}\n",
    "    for line in f_in:\n",
    "        tokens = line.strip().split(' ')\n",
    "        if tokens[0] in vocabulary:\n",
    "            vectors[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(train_data, vocab_size):\n",
    "    '''\n",
    "    Returns:\n",
    "    id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    '''\n",
    "    print('Building vocabulary... ', end='', flush=True)\n",
    "    all_tokens = []\n",
    "    for row in (train_data['sentence1']+train_data['sentence2']).iteritems():\n",
    "        all_tokens += row[1]\n",
    "    vocabulary, count = zip(*Counter(all_tokens).most_common(vocab_size))\n",
    "    print('Done.')\n",
    "    print('Loading vectors... ', end='', flush=True)\n",
    "    vectors = load_vectors('data/wiki-news-300d-1M.vec', vocabulary)\n",
    "    vocabulary = [word for word in vocabulary if word in vectors]\n",
    "    print('Done.')\n",
    "    id2token = list(vocabulary)\n",
    "    token2id = dict(zip(vocabulary, range(2, 2+len(vocabulary))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token, vectors\n",
    "\n",
    "def read_and_preprocess_data(data_dict, dataset, vocab_size=50000):\n",
    "    data = prepare_data(data_dict[dataset])\n",
    "    if dataset == 'train':\n",
    "        token2id, id2token, vectors = build_vocabulary(data, vocab_size)\n",
    "        return data, token2id, id2token, vectors\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary... Done.\n",
      "Loading vectors... Done.\n"
     ]
    }
   ],
   "source": [
    "train_data, token2id, id2token, vectors = read_and_preprocess_data(data_dict, 'train', VOCAB_SIZE)\n",
    "val_data = read_and_preprocess_data(data_dict, 'val')\n",
    "\n",
    "pickle.dump(vectors, open('data/vectors.pkl', 'wb'))\n",
    "pickle.dump(token2id, open('data/token2id.pkl', 'wb'))\n",
    "pickle.dump(id2token, open('data/id2token.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(train_data)\n",
    "val_data = prepare_data(val_data)\n",
    "vectors = pickle.load(open('data/vectors.pkl', 'rb'))\n",
    "id2token = pickle.load(open('data/id2token.pkl', 'rb'))\n",
    "token2id = pickle.load(open('data/token2id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22059"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id: 6038; Token: applied\n",
      "Token: applied; Token id: 6038\n"
     ]
    }
   ],
   "source": [
    "# Check the dictionary by loading random token from it\n",
    "random_token_id = np.random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "print(\"Token id: {}; Token: {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print(\"Token: {}; Token id: {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, token2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.x1, self.x2, self.y = data['sentence1'].values, data['sentence2'].values, data['label'].values\n",
    "        assert (len(self.x1) == len(self.x2) == len(self.y))\n",
    "        self.token2id = token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, row):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        label = self.y[row]\n",
    "        x1_word_idx, x2_word_idx = [], []\n",
    "        x1_mask, x2_mask = [], []\n",
    "        \n",
    "        for word in self.x1[row][:MAX_SENT_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                x1_word_idx.append(self.token2id[word])\n",
    "                x1_mask.append(0)\n",
    "            else:\n",
    "                x1_word_idx.append(UNK_IDX)\n",
    "                x1_mask.append(1)\n",
    "                \n",
    "        for word in self.x2[row][:MAX_SENT_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                x2_word_idx.append(self.token2id[word])\n",
    "                x2_mask.append(0)\n",
    "            else:\n",
    "                x2_word_idx.append(UNK_IDX)\n",
    "                x2_mask.append(1)\n",
    "        \n",
    "        x1_list = [x1_word_idx, x1_mask, len(x1_word_idx)]\n",
    "        x2_list = [x2_word_idx, x2_mask, len(x2_word_idx)]\n",
    "        return x1_list + x2_list + [label]\n",
    "\n",
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    x1_data, x2_data = [], []\n",
    "    x1_mask, x2_mask = [], []\n",
    "    x1_lengths, x2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        x1_lengths.append(datum[2])\n",
    "        x2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "        \n",
    "        # Padding\n",
    "        x1_data_padded = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0, MAX_SENT_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x1_data.append(x1_data_padded)\n",
    "        \n",
    "        x1_mask_padded = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0, MAX_SENT_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x1_mask.append(x1_mask_padded)\n",
    "        \n",
    "        x2_data_padded = np.pad(np.array(datum[3]),\n",
    "                                pad_width=((0, MAX_SENT_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x2_data.append(x2_data_padded)\n",
    "        \n",
    "        x2_mask_padded = np.pad(np.array(datum[4]),\n",
    "                               pad_width=((0, MAX_SENT_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        x2_mask.append(x2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(x1_lengths)[::-1]\n",
    "    x1_data = np.array(x1_data)[ind_dec_order]\n",
    "    x2_data = np.array(x2_data)[ind_dec_order]\n",
    "    \n",
    "    x1_mask = np.array(x1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    x2_mask = np.array(x2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    \n",
    "    x1_lengths = np.array(x1_lengths)[ind_dec_order]\n",
    "    x2_lengths = np.array(x2_lengths)[ind_dec_order]\n",
    "    \n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    x1_list = [torch.from_numpy(x1_data), torch.from_numpy(x1_mask).float(), x1_lengths]\n",
    "    x2_list = [torch.from_numpy(x2_data), torch.from_numpy(x2_mask).float(), x2_lengths]\n",
    "        \n",
    "    return x1_list + x2_list + [torch.from_numpy(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_sentence_length(train_data, q=0.95):\n",
    "    max_sent1_len = train_data['sentence1'].str.len().quantile(q)\n",
    "    max_sent2_len = train_data['sentence2'].str.len().quantile(q)\n",
    "    return int(max(max_sent1_len, max_sent2_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = get_max_sentence_length(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "\n",
    "train_dataset = SNLIDataset(train_data, token2id)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          collate_fn=snli_collate_func,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset = SNLIDataset(val_data, token2id)\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        collate_fn=snli_collate_func,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise weights matrix\n",
    "def init_embedding_weights(vectors, token2id, id2token):\n",
    "    weights = np.zeros((len(token2id), 300))\n",
    "    for idx in range(2, len(vectors)):\n",
    "        weights[idx] = np.array(vectors[id2token[idx]])\n",
    "    np.random.seed(1337)\n",
    "    weights[1] = np.random.randn(300)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, weights, vocab_size, bidirectional=False):\n",
    "        '''\n",
    "        params:\n",
    "            hidden_size: hidden Size of layer in GRU\n",
    "            num_layers: number of layers in GRU\n",
    "            output_size: dimension of output\n",
    "            vocab_size: vocabulary size\n",
    "            bidirectional: use bidirectional GRU\n",
    "        '''\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=PAD_IDX)\n",
    "        self.gru = nn.GRU(300, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights))\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        true2sorted = sorted(range(len(lengths)), key=lambda x: -lengths[x])\n",
    "        sorted2true = sorted(range(len(lengths)), key=lambda x: true2sorted[x])\n",
    "        x = x[true2sorted]\n",
    "        mask = mask[true2sorted]\n",
    "        lengths = lengths[true2sorted]\n",
    "        \n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # reset hidden state\n",
    "        self.hidden = self._init_hidden(batch_size)\n",
    "        \n",
    "        # get embedding of words\n",
    "        embed = self.embedding(x)\n",
    "        \n",
    "        # mask out all embeddings other than <unk> token to freeze their weights\n",
    "        embed = mask*embed + (1-mask)*embed.clone().detach()\n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        \n",
    "        # forward prop though GRU\n",
    "        gru_out, self.hidden = self.gru(embed, self.hidden)\n",
    "        \n",
    "        # undo packing\n",
    "        gru_out, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out, batch_first=True)\n",
    "        \n",
    "        # (batch_size, seq_len, num_directions*hidden_size) -> (batch_size, seq_len, num_directions, hidden_size)\n",
    "        gru_out = gru_out.view(batch_size, -1, self.num_directions, self.hidden_size)\n",
    "        \n",
    "        # sum hidden activations of GRU across time\n",
    "        gru_out = torch.sum(gru_out, dim=1)\n",
    "        \n",
    "        # concat all directions along the hidden dimension\n",
    "        gru_out = torch.cat([gru_out[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "        \n",
    "        # get data back in original order of batches\n",
    "        gru_out = gru_out[sorted2true]\n",
    "        \n",
    "        return gru_out\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_directions, hidden_size, num_outputs, mode='cat'):\n",
    "        super(ClassificationNetwork, self).__init__()\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        # Fully connected and ReLU layers\n",
    "        if mode == 'cat':\n",
    "            self.fc1 = nn.Linear(2*input_size*num_directions, hidden_size)\n",
    "        elif mode in ['elementwise_mult', 'sum']:\n",
    "            self.fc1 = nn.Linear(input_size*num_directions, hidden_size)\n",
    "        else:\n",
    "            raise ValueError('Invalid arugment \"{}\" for mode!'.format(mode))\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, embedding_output1, embedding_output2):\n",
    "        if self.mode == 'cat':\n",
    "            input = torch.cat([embedding_output1, embedding_output2], dim=1)\n",
    "        elif self.mode == 'elementwise_mult':\n",
    "            input = embedding_output1 * embedding_output2\n",
    "        elif self.mode == 'sum':\n",
    "            input = embedding_output1 + embedding_output2\n",
    "        input = input.view(input.size(0), -1) # Reshape input to batch_size x num_inputs\n",
    "        output = self.fc1(input)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.uniform_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data_loader, embedding_network, classification_network):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    embedding_network.eval()\n",
    "    classification_network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x1, x1_mask, x1_lengths, x2, x2_mask, x2_lengths, y) in enumerate(data_loader):\n",
    "            x1, x1_mask, x2, x2_mask, y = x1.to(DEVICE), x1_mask.to(DEVICE), x2.to(DEVICE), x2_mask.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            embedding_output1 = embedding_network(x1, x1_mask, x1_lengths)\n",
    "            embedding_output2 = embedding_network(x2, x2_mask, x2_lengths)\n",
    "            classification_output = classification_network(embedding_output1, embedding_output2)\n",
    "            \n",
    "            output_normalized = F.softmax(classification_output, dim=1)\n",
    "            predicted = output_normalized.max(1, keepdim=True)[1]\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = init_embedding_weights(vectors, token2id, id2token)\n",
    "embedding_network = GRUEncoder(hidden_size=EMB_HIDDEN_SIZE, num_layers=NUM_LAYERS, weights=WEIGHTS, vocab_size=len(token2id), bidirectional=BIDIRECTIONAL).to(DEVICE)\n",
    "classification_network = ClassificationNetwork(EMB_HIDDEN_SIZE, NUM_DIRECTIONS, CLASS_HODDEN_SIZE, NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(embedding_network.parameters())+list(classification_network.parameters()), lr=LR)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 36.3\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 35.6\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 39.0\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 38.6\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 38.5\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 37.1\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 39.4\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 40.2\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 39.4\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 38.3\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 39.2\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 39.2\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 40.4\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 41.0\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 38.8\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 43.2\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 42.4\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 41.6\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 42.0\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 43.3\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 44.2\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 43.0\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 42.5\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 46.3\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 44.9\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 45.3\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 46.4\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 46.3\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 49.0\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 46.9\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 49.5\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 51.4\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 51.1\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 48.7\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 50.9\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 52.6\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 54.1\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 51.9\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 52.7\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 54.9\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 53.6\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 53.1\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 53.9\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 53.1\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 51.0\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 52.9\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 53.0\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 54.8\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 52.7\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 54.0\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 56.6\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 56.8\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 54.2\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 55.3\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 54.5\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 57.6\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 56.4\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 58.0\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 57.5\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 57.2\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 58.0\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 57.7\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 56.8\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 57.0\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 56.6\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 58.3\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 56.3\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 54.5\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 57.9\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 57.2\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 57.3\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 58.5\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 56.8\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 56.3\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 58.3\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 57.5\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 58.0\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 57.6\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 57.9\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 58.3\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 59.1\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 57.2\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 59.1\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 56.4\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 60.8\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 59.2\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 57.1\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 57.6\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 59.4\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 59.8\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 59.2\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 58.5\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 60.6\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 60.9\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 60.1\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 60.2\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 61.3\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 59.8\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 61.1\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 60.6\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 60.5\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 62.2\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 59.3\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 61.6\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 62.3\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 61.6\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 60.2\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 60.4\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 61.7\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 62.2\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 63.2\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 61.7\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 60.7\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 62.5\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 61.7\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 62.0\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 61.7\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 63.0\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 62.2\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 61.6\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 61.6\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 62.5\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 62.6\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 63.1\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 63.5\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 61.7\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 62.0\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 63.1\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 62.3\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 65.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 62.6\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 63.2\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 65.6\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 63.5\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 65.0\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 64.8\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 66.7\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 66.9\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 67.0\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 65.4\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 65.6\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 68.3\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 68.4\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 67.0\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 67.9\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 67.6\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 67.6\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 67.2\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 67.2\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 67.7\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 68.0\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 68.5\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 68.2\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 67.5\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 67.7\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 68.4\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 68.6\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 68.3\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 69.5\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 69.1\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 69.4\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 68.4\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 70.2\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 69.0\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 69.7\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 69.0\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 69.0\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 68.3\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 69.6\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 68.8\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 68.8\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 69.5\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 69.8\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 69.8\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 69.5\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 68.5\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 67.5\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 68.6\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 69.6\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 68.0\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 69.5\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 68.8\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 66.7\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 70.4\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 70.0\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 70.2\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 69.6\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 70.4\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 69.4\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 70.1\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 70.4\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 69.3\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 70.6\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 70.6\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 68.7\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 69.4\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 69.8\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 69.3\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 69.2\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 69.8\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 69.2\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 69.3\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 69.9\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 70.1\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 70.7\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 70.0\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 70.4\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 68.7\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 70.3\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 70.4\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 71.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 71.5\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 69.6\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 70.9\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 69.3\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 70.6\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 69.7\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 71.0\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 69.2\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 69.9\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 71.2\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    for batch_idx, (x1, x1_mask, x1_lengths, x2, x2_mask, x2_lengths, y) in enumerate(train_loader):\n",
    "        x1, x1_mask, x2, x2_mask, y = x1.to(DEVICE), x1_mask.to(DEVICE), x2.to(DEVICE), x2_mask.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        embedding_network.train()\n",
    "        classification_network.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        embedding_output1 = embedding_network(x1, x1_mask, x1_lengths)\n",
    "        embedding_output2 = embedding_network(x2, x2_mask, x2_lengths)\n",
    "        classification_output = classification_network(embedding_output1, embedding_output2)\n",
    "        loss = criterion(classification_output, y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validate every 100 iterations\n",
    "        if batch_idx > 0 and batch_idx % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, embedding_network, classification_network)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                epoch+1, N_EPOCHS, batch_idx+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNNEncoder, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "        logits = self.linear(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things to keep in mind when using Convolutional Nets for Language Tasks in Pytorch\n",
    "\n",
    "### Conv1d module expect input of size (batch_size, num_channels, length), where in our case input has size (batch_size, length, num_channels). Hence it is important call transpose(1,2) before passing it to convolutional layer and then reshape it back to (batch_size, length, num_channels) by calling transpose(1,2) again\n",
    "\n",
    "### Additionally we need to reshape hidden activations into 2D tensor before passing it to Relu layer by calling view(-1, hidden.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNEncoder(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:capstone_project]",
   "language": "python",
   "name": "conda-env-capstone_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
