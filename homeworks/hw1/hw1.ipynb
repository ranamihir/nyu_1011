{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "PROJECT_DIR = '/home/mihir/Desktop/GitHub/nyu/nyu_1011/homeworks/hw1/'\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "PLOTS_DIR = os.path.join(PROJECT_DIR, 'plots')\n",
    "NUM_VAL = 5000\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_SIZE = 64     # input batch size for training\n",
    "N_EPOCHS = 50       # number of epochs to train\n",
    "LR = 0.01           # learning rate\n",
    "VOCAB_SIZE = 10000  # max vocab size\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "EMB_DIM = 100       # size of embedding\n",
    "\n",
    "\n",
    "# Save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset='train'):\n",
    "    data_path = os.path.join(DATA_DIR, dataset)\n",
    "    data = []\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        target = 1 if sentiment == 'pos' else 0\n",
    "        data_target_path = os.path.join(data_path, sentiment)\n",
    "        for file in os.listdir(data_target_path):\n",
    "            file_path = os.path.join(data_target_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r') as file_text:\n",
    "                    text = file_text.readlines()[0].replace(')', ' ').replace('(', ' ')\n",
    "                    text = re.sub('<[^<]+?>', '', text)\n",
    "                    data.append([text, target])\n",
    "    data = pd.DataFrame(data, columns=['text', 'sentiment'])\n",
    "    data['text'] = data['text'].astype(str)\n",
    "    data['sentiment'] = data['sentiment'].astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train_data):\n",
    "    train_data.sample(frac=1, random_state=1337)\n",
    "    val_data = train_data[:NUM_VAL]\n",
    "    train_data = train_data[NUM_VAL:]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_val_datasets(force=False):\n",
    "    train_data_path = os.path.join(DATA_DIR, 'train.pkl')\n",
    "    val_data_path = os.path.join(DATA_DIR, 'val.pkl')\n",
    "    if not force and os.path.exists(train_data_path) and os.path.exists(val_data_path):\n",
    "        train_data = pickle.load(open(train_data_path, 'rb'))\n",
    "        val_data = pickle.load(open(val_data_path, 'rb'))\n",
    "    else:\n",
    "        train_data = load_dataset('train')\n",
    "        train_data, val_data = split_train_val(train_data)\n",
    "        pickle.dump(train_data, open(train_data_path, 'wb'))\n",
    "        pickle.dump(val_data, open(val_data_path, 'wb'))\n",
    "    return train_data.reset_index(drop=True), val_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(force=False):\n",
    "    test_data_path = os.path.join(DATA_DIR, 'test.pkl')\n",
    "    if not force and os.path.exists(test_data_path):\n",
    "        test_data = pickle.load(open(test_data_path, 'rb'))\n",
    "    else:\n",
    "        test_data = load_dataset('test')\n",
    "        pickle.dump(test_data, open(test_data_path, 'wb'))\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = load_train_val_datasets()\n",
    "test_data = load_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size is {}\".format(len(train_data)))\n",
    "print(\"Val dataset size is {}\".format(len(val_data)))\n",
    "print(\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text         It had all the clich√©s of movies of this type ...\n",
      "sentiment                                                    0\n",
      "Name: 16444, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Random sample from train dataset\n",
    "print(train_data.iloc[np.random.randint(0, len(train_data)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stopwords():\n",
    "    NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\"no\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "    stopwords = STOP_WORDS.copy()\n",
    "    for word in STOP_WORDS:\n",
    "        if word in NEGATE:\n",
    "            stopwords.remove(word)\n",
    "\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tokens, stopwords, punctuations):\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in tokens]\n",
    "    tokens = [unidecode(tok) for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer+tagger+parser+NER+word vectors, and punctuations and stopwords\n",
    "tokenizer = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner'])\n",
    "punctuations = string.punctuation\n",
    "stopwords = prepare_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data, tokenizer, stopwords, punctuations, dataset='train', force=False):\n",
    "    tokens_data_path = os.path.join(DATA_DIR, '{}_tokenized.pkl'.format(dataset))\n",
    "    all_train_tokens_path = os.path.join(DATA_DIR, 'all_train_tokens.pkl')\n",
    "    if not force and os.path.exists(tokens_data_path):\n",
    "        tokens_data = pickle.load(open(tokens_data_path, 'rb'))\n",
    "        if dataset == 'train':\n",
    "            all_train_tokens = pickle.load(open(all_train_tokens_path, 'rb'))\n",
    "            return tokens_data, all_train_tokens\n",
    "        return tokens_data\n",
    "    else:\n",
    "        parsed_data = tokenizer.pipe(data['text'], batch_size=512, n_threads=-1)\n",
    "        tokens_data = pd.Series(parsed_data).apply(clean_data, args=(stopwords, punctuations))\n",
    "        pickle.dump(tokens_data, open(tokens_data_path, 'wb'))\n",
    "        if dataset == 'train':\n",
    "            all_train_tokens = np.hstack(tokens_data)\n",
    "            pickle.dump(all_train_tokens, open(all_train_tokens_path, 'wb'))\n",
    "            return tokens_data, all_train_tokens\n",
    "    return tokens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data, tokenizer, stopwords, punctuations, dataset='train')\n",
    "val_data_tokens = tokenize_dataset(val_data, tokenizer, stopwords, punctuations, dataset='val')\n",
    "test_data_tokens = tokenize_dataset(test_data, tokenizer, stopwords, punctuations, dataset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [film, watch, high, school, spanish, class, fa...\n",
       "1    [know, absolutely, nothing, ireland, love, lef...\n",
       "2    [watch, star, josie, lawrence, knewfrom, line,...\n",
       "3    [example, film, not, good, receive, stand, rai...\n",
       "4    [horror, not, educational, film, genre, huh, t...\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['film', 'watch', 'high', ..., 'wrong', 'val', 'kilmer'],\n",
       "      dtype='<U74')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in train dataset = 2175805\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of tokens in train dataset = {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(all_train_tokens, vocab_size):\n",
    "    '''\n",
    "    Returns:\n",
    "    id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    '''\n",
    "    \n",
    "    token_counter = Counter(all_train_tokens)\n",
    "    vocabulary, count = zip(*token_counter.most_common(vocab_size))\n",
    "    id2token = list(vocabulary)\n",
    "    token2id = dict(zip(vocabulary, range(2, 2+len(vocabulary)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocabulary(all_train_tokens, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 2566 ; token usa\n",
      "Token usa; token id 2566\n"
     ]
    }
   ],
   "source": [
    "# Check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = np.random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print(\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print(\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = tokens_data.apply(lambda tokens: [token2id[token] if token in token2id else UNK_IDX \\\n",
    "                                                     for token in tokens])\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of review tokens\n",
    "        @param target_list: list of review targets\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when dataset[i] is called\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdbreviews_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                            pad_width=((0, MAX_SENTENCE_LENGTH-datum[1])),\n",
    "                            mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBReviewsDataset(train_data_indices, train_data['sentiment'])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdbreviews_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBReviewsDataset(val_data_indices, val_data['sentiment'])\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         collate_fn=imdbreviews_collate_func,\n",
    "                                         shuffle=True)\n",
    "\n",
    "test_dataset = IMDBReviewsDataset(test_data_indices, test_data['sentiment'])\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          collate_fn=imdbreviews_collate_func,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary\n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "\n",
    "        # Pay attention to padding_idx\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "\n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a\n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0], 1).expand_as(out).float()\n",
    "\n",
    "        # Return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Criterion, and Optimizer\n",
    "model = BagOfWords(len(id2token), EMB_DIM).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(dataloader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_batch, lengths_batch, labels_batch in dataloader:\n",
    "            data_batch, lengths_batch, labels_batch = data_batch.to(DEVICE), lengths_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            outputs = nn.functional.softmax(model(data_batch, lengths_batch), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            total += labels_batch.size(0)\n",
    "            correct += predicted.eq(labels_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_training(model, train_loader, val_loader, criterion, optimizer, n_epochs):\n",
    "    train_loss_history, val_accuracies = [], []\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        for batch_idx, (data_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "            data_batch, lengths_batch, labels_batch = data_batch.to(DEVICE), lengths_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, lengths_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_history.append(loss.item())\n",
    "\n",
    "            if batch_idx == len(train_loader)-1:    # validate every 100 iterations\n",
    "                val_accuracy = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Accuracy: {}'\\\n",
    "                      .format(epoch, n_epochs, batch_idx+1, len(train_loader), val_accuracy))\n",
    "                val_accuracies.append(val_accuracy)\n",
    "    return train_loss_history, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_history, val_accuracies = run_training(model, train_loader, val_loader, criterion, optimizer, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = pd.DataFrame({\n",
    "    'train': train_loss_history\n",
    "})\n",
    "train_loss_history.plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies = pd.DataFrame({\n",
    "    'val': val_accuracies\n",
    "})\n",
    "val_accuracies.plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After training for {} epochs:\".format(N_EPOCHS))\n",
    "print(\"Val Accuracy: {}\".format(test_model(val_loader, model)))\n",
    "print(\"Test Accuracy: {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZEs = pd.DataFrame([16, 32, 64, 128], columns=['batch_size'])\n",
    "BATCH_SIZEs = pd.DataFrame([64], columns=['batch_size'])\n",
    "LRs = pd.DataFrame([1e-2, 1e-3, 1e-5], columns=['lr'])\n",
    "EMB_DIMs = pd.DataFrame([100, 256, 512, 800, 1024], columns=['emb_dim'])\n",
    "VOCAB_SIZEs = pd.DataFrame([10000, 20000, 50000], columns=['vocab_size'])\n",
    "OPTIMIZERS = pd.DataFrame(['adam', 'sgd'], columns=['optimizer'])\n",
    "# MAX_SENTENCE_LENGTHs = pd.DataFrame([100, 256, 512], columns=['max_sent_length'])\n",
    "MAX_SENTENCE_LENGTHs = pd.DataFrame([100], columns=['max_sent_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning():\n",
    "    try:\n",
    "        cv_results = pd.DataFrame(columns=['batch_size', 'lr', 'emb_dim', 'vocab_size', \\\n",
    "                                           'max_sent_length', 'optimizer', \\\n",
    "                                           'train_loss_hist', 'val_accuracies', 'max_accuracy'])\n",
    "\n",
    "        params = pd.DataFrame([1]*len(EMB_DIMs), columns=['key'])\n",
    "        for df in BATCH_SIZEs, LRs, EMB_DIMs, VOCAB_SIZEs, MAX_SENTENCE_LENGTHs, OPTIMIZERS:\n",
    "            df['key'] = 1\n",
    "            params = pd.merge(params, df, on='key')\n",
    "        params = params.drop('key', axis=1).drop_duplicates()\n",
    "\n",
    "        for row in params.iterrows():\n",
    "            print('\\n', params.iloc[row[0]:row[0]+1])\n",
    "\n",
    "            batch_size, emb_dim, vocab_size, max_sent_length = int(row[1]['batch_size']), \\\n",
    "                int(row[1]['emb_dim']), int(row[1]['vocab_size']), int(row[1]['max_sent_length'])\n",
    "            lr = row[1]['lr']\n",
    "\n",
    "            token2id, id2token = build_vocabulary(all_train_tokens, vocab_size)\n",
    "\n",
    "            train_data_indices = token2index_dataset(train_data_tokens)\n",
    "            val_data_indices = token2index_dataset(val_data_tokens)\n",
    "\n",
    "            train_dataset = IMDBReviewsDataset(train_data_indices, train_data['sentiment'])\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       collate_fn=imdbreviews_collate_func,\n",
    "                                                       shuffle=True)\n",
    "\n",
    "            val_dataset = IMDBReviewsDataset(val_data_indices, val_data['sentiment'])\n",
    "            val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     collate_fn=imdbreviews_collate_func,\n",
    "                                                     shuffle=True)\n",
    "\n",
    "            model = BagOfWords(len(id2token), emb_dim).to(DEVICE)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = {'adam': torch.optim.Adam(model.parameters(), lr=lr), \\\n",
    "                          'sgd': torch.optim.SGD(model.parameters(), lr=lr)}[row[1]['optimizer']]\n",
    "                \n",
    "            train_loss_history, val_accuracies = run_training(model, train_loader, val_loader, \\\n",
    "                                                              criterion, optimizer, N_EPOCHS)\n",
    "            max_accuracy = np.max(val_accuracies)\n",
    "\n",
    "            result = pd.DataFrame([[batch_size, lr, emb_dim, vocab_size, max_sent_length, train_loss_history, \\\n",
    "                                    val_accuracies, max_accuracy]], columns=cv_results.columns)\n",
    "            cv_results = cv_results.append(result)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        return cv_results\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 6)\n",
      "> <ipython-input-32-7759add29f4a>(13)hyperparameter_tuning()\n",
      "-> for row in params.iterrows():\n",
      "(Pdb) params.head()\n",
      "   batch_size    lr  emb_dim  vocab_size  max_sent_length optimizer\n",
      "0          64  0.01      100       10000              100      adam\n",
      "1          64  0.01      100       10000              100       sgd\n",
      "2          64  0.01      100       20000              100      adam\n",
      "3          64  0.01      100       20000              100       sgd\n",
      "4          64  0.01      100       50000              100      adam\n",
      "(Pdb) exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b60b34fcd820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-7759add29f4a>\u001b[0m in \u001b[0;36mhyperparameter_tuning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-7759add29f4a>\u001b[0m in \u001b[0;36mhyperparameter_tuning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_results = hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = hyperparameter_tuning()\n",
    "pickle.dump(cv_results, open(os.path.join(DATA_DIR, 'cv_results.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
